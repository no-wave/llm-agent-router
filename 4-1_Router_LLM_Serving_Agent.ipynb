{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Router LLM Serving Agent 튜토리얼\n",
    "\n",
    "\n",
    "Router LLM Serving Agent는 클라우드 API 모델과 로컬 모델 간의 라우팅을 자동화하는 시스템이다. 네트워크 상태, 비용, 개인정보 보호 수준에 따라 OpenAI의 클라우드 모델(gpt-4o-mini)과 Ollama의 로컬 모델 중 최적의 선택을 한다.\n",
    "\n",
    "기본 작동 원리는 다음과 같다:\n",
    "1. 질문의 특성을 분석한다 (민감도, 복잡도)\n",
    "2. 시스템 상태를 확인한다 (네트워크 상태, 로컬 모델 가용성)\n",
    "3. 분석 결과에 따라 클라우드 또는 로컬 모델을 선택한다\n",
    "4. 선택된 모델로 질문에 답변한다\n",
    "\n",
    "이 시스템은 비용 절감, 응답 속도 개선, 개인정보 보호를 동시에 달성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env 파일에서 환경 변수를 로드한다\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# OpenAI 클라이언트 초기화 (클라우드 모델)\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Ollama 로컬 서버 주소\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "model_config = {\n",
    "    \"cloud\": {\n",
    "        \"name\": \"gpt-4o-mini\",\n",
    "        \"type\": \"cloud\",\n",
    "        \"description\": \"OpenAI 클라우드 모델\",\n",
    "        \"advantages\": [\"높은 성능\", \"안정적인 서비스\", \"복잡한 추론 가능\"]\n",
    "    },\n",
    "    \"local\": {\n",
    "        \"name\": \"exaone3.5:7.8b\",\n",
    "        \"type\": \"local\",\n",
    "        \"description\": \"Ollama 로컬 모델\",\n",
    "        \"advantages\": [\"비용 절감\", \"빠른 응답\", \"개인정보 보호\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_local_model_available():\n",
    "    \"\"\"로컬 Ollama 모델이 사용 가능한지 확인한다\"\"\"\n",
    "    \n",
    "    # Ollama 서버에 연결 시도\n",
    "    response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
    "    \n",
    "    # 상태 코드가 200이면 사용 가능\n",
    "    available = response.status_code == 200\n",
    "    \n",
    "    return available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query_sensitivity(query):\n",
    "    \"\"\"질문의 민감도를 분석한다\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    다음 질문의 민감도를 분석하라.\n",
    "    \n",
    "    질문: {query}\n",
    "    \n",
    "    민감도 기준:\n",
    "    - low: 일반적인 정보, 공개된 지식\n",
    "    - medium: 개인적인 의견, 약간 민감한 주제\n",
    "    - high: 개인정보, 기밀 정보, 매우 민감한 주제\n",
    "    \n",
    "    민감도만 답변하라 (low, medium, high 중 하나).\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    sensitivity = response.choices[0].message.content.strip().lower()\n",
    "    return sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_model(query):\n",
    "    \"\"\"질문 특성과 시스템 상태를 고려하여 사용할 모델을 결정한다\"\"\"\n",
    "    \n",
    "    # 1. 로컬 모델 사용 가능 여부 확인\n",
    "    local_available = check_local_model_available()\n",
    "    print(f\"로컬 모델 사용 가능: {local_available}\")\n",
    "    \n",
    "    # 2. 질문 민감도 분석\n",
    "    sensitivity = analyze_query_sensitivity(query)\n",
    "    print(f\"질문 민감도: {sensitivity}\")\n",
    "    \n",
    "    # 3. 라우팅 결정\n",
    "    # 로컬 모델이 없으면 무조건 클라우드\n",
    "    selected_type = \"cloud\"\n",
    "    reason = \"로컬 모델 사용 불가\"\n",
    "    \n",
    "    # 로컬 모델이 있는 경우\n",
    "    # 민감도가 높으면 로컬 모델 사용 (개인정보 보호)\n",
    "    selected_type = \"local\"\n",
    "    reason = \"개인정보 보호를 위한 로컬 처리\"\n",
    "    \n",
    "    # 민감도가 낮거나 중간이면 클라우드 모델 사용 (높은 성능)\n",
    "    selected_type = \"cloud\"\n",
    "    reason = \"높은 성능이 필요한 일반 질문\"\n",
    "    \n",
    "    return selected_type, reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cloud_model(query):\n",
    "    \"\"\"OpenAI 클라우드 모델에 질문한다\"\"\"\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_local_model(query):\n",
    "    \"\"\"Ollama 로컬 모델에 질문한다\"\"\"\n",
    "    \n",
    "    # Ollama API 요청\n",
    "    payload = {\n",
    "        \"model\": \"exaone3.5:7.8b\",\n",
    "        \"prompt\": query,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_BASE_URL}/api/generate\",\n",
    "        json=payload\n",
    "    )\n",
    "    \n",
    "    result = response.json()\n",
    "    answer = result.get(\"response\", \"\")\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_model(model_type, query):\n",
    "    \"\"\"선택된 모델 타입에 따라 적절한 모델로 라우팅한다\"\"\"\n",
    "    \n",
    "    # 클라우드 또는 로컬 모델 선택\n",
    "    answer = query_cloud_model(query)\n",
    "    \n",
    "    answer = query_local_model(query)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_serving_agent(query):\n",
    "    \"\"\"Router LLM Serving Agent의 전체 파이프라인을 실행한다\"\"\"\n",
    "    \n",
    "    print(f\"질문: {query}\\n\")\n",
    "    \n",
    "    # 1단계: 모델 선택 결정\n",
    "    model_type, reason = decide_model(query)\n",
    "    selected_model = model_config[model_type]\n",
    "    \n",
    "    print(f\"\\n선택된 모델 타입: {model_type}\")\n",
    "    print(f\"모델 이름: {selected_model['name']}\")\n",
    "    print(f\"선택 이유: {reason}\")\n",
    "    print(f\"모델 장점: {', '.join(selected_model['advantages'])}\\n\")\n",
    "    \n",
    "    # 2단계: 선택된 모델로 질문 처리\n",
    "    answer = route_to_model(model_type, query)\n",
    "    \n",
    "    print(f\"답변:\\n{answer}\\n\")\n",
    "    \n",
    "    # 결과 반환\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"model_type\": model_type,\n",
    "        \"model_name\": selected_model['name'],\n",
    "        \"reason\": reason,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질문: Python에서 리스트와 튜플의 차이는?\n",
      "\n",
      "로컬 모델 사용 가능: True\n",
      "질문 민감도: low\n",
      "\n",
      "선택된 모델 타입: cloud\n",
      "모델 이름: gpt-4o-mini\n",
      "선택 이유: 높은 성능이 필요한 일반 질문\n",
      "모델 장점: 높은 성능, 안정적인 서비스, 복잡한 추론 가능\n",
      "\n",
      "답변:\n",
      "Python에서 리스트와 튜플은 모두 순서가 있는 데이터 컬렉션을 나타내지만, 몇 가지 중요한 차이점이 있습니다:\n",
      "\n",
      "### 1. **변경 가능성 (Mutability)**\n",
      "- **리스트 (List):**\n",
      "  - **변경 가능 (Mutable):** 리스트의 요소를 추가, 삭제, 수정할 수 있습니다.\n",
      "  - 예시:\n",
      "    ```python\n",
      "    my_list = [1, 2, 3]\n",
      "    my_list[0] = 10  # 요소 수정 가능\n",
      "    my_list.append(4)  # 요소 추가 가능\n",
      "    del my_list[2]  # 요소 삭제 가능\n",
      "    ```\n",
      "\n",
      "- **튜플 (Tuple):**\n",
      "  - **불변 (Immutable):** 튜플의 요소는 생성 후 수정, 삭제할 수 없습니다.\n",
      "  - 예시:\n",
      "    ```python\n",
      "    my_tuple = (1, 2, 3)\n",
      "    # my_tuple[0] = 10  # 오류 발생: TypeError: 'tuple' object does not support item assignment\n",
      "    ```\n",
      "\n",
      "### 2. **성능**\n",
      "- **튜플:** 일반적으로 리스트보다 메모리 사용량이 적고, 성능이 빠를 수 있습니다 (특히 큰 데이터셋에서).\n",
      "- **리스트:** 동적 크기 조정이 가능하여 유연성이 높습니다.\n",
      "\n",
      "### 3. **사용 목적**\n",
      "- **리스트:** 데이터가 변경될 가능성이 있는 상황에서 사용합니다 (예: 사용자 입력 처리, 데이터 임시 저장 등).\n",
      "- **튜플:** 데이터가 변경되지 않는 상황에서 사용합니다 (예: 좌표, 레코드 저장 등). 불변성 덕분에 코드 안전성을 높일 수 있습니다.\n",
      "\n",
      "### 요약 테이블\n",
      "| 특성         | 리스트 (List)                      | 튜플 (Tuple)                    |\n",
      "|--------------|------------------------------------|---------------------------------|\n",
      "| 변경 가능성   | 변경 가능 (Mutable)                | 불변 (Immutable)               |\n",
      "| 메모리 사용량 | 보통                            | 보통 더 적음                   |\n",
      "| 성능         | 보통                            | 보통 더 빠름                   |\n",
      "| 사용 사례    | 데이터 변경이 예상되는 경우        | 데이터 변경이 없는 경우        |\n",
      "\n",
      "이러한 차이점을 고려하여 프로그래밍 상황에 맞는 자료구조를 선택하면 효율적인 코드 작성이 가능합니다.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "질문: 우리 회사의 매출 데이터를 분석해줘\n",
      "\n",
      "로컬 모델 사용 가능: True\n",
      "질문 민감도: medium\n",
      "\n",
      "선택된 모델 타입: cloud\n",
      "모델 이름: gpt-4o-mini\n",
      "선택 이유: 높은 성능이 필요한 일반 질문\n",
      "모델 장점: 높은 성능, 안정적인 서비스, 복잡한 추론 가능\n",
      "\n",
      "답변:\n",
      "죄송합니다만, 저는 텍스트 기반 AI 모델이라 직접적인 데이터 분석이나 시각화 기능은 제공하지 못합니다. 하지만 매출 데이터 분석을 위한 다음과 같은 단계와 고려사항들을 제안드릴 수 있습니다:\n",
      "\n",
      "1. **데이터 수집 및 정리**:\n",
      "   - 가장 최신의 매출 데이터를 수집합니다.\n",
      "   - 데이터 형식을 일관되게 정리합니다 (예: 날짜, 상품/서비스별 매출, 비용 등).\n",
      "\n",
      "2. **기본 통계 분석**:\n",
      "   - 총 매출, 평균 매출, 최대/최소 매출 등 기본적인 통계를 확인합니다.\n",
      "   - 월별, 분기별, 연도별 매출 추이를 살펴봅니다.\n",
      "\n",
      "3. **트렌드 분석**:\n",
      "   - 시간에 따른 매출 변화의 패턴을 찾아봅니다 (상승 추세, 하락 추세 등).\n",
      "   - 특정 이벤트나 계절성이 매출에 미치는 영향을 분석합니다.\n",
      "\n",
      "4. **상세 분석**:\n",
      "   - 각 제품 또는 서비스별 매출을 분석하여 수익성이 높은 항목과 낮은 항목을 파악합니다.\n",
      "   - 경쟁사와의 비교 분석을 통해 시장 위치를 이해합니다.\n",
      "\n",
      "5. **예측 분석**:\n",
      "   - 과거 데이터를 기반으로 향후 매출을 예측하기 위해 선형 회귀, 시계열 분석 등의 기법을 사용할 수 있습니다.\n",
      "\n",
      "이러한 분석을 수행하기 위해서는 실제 데이터와 함께 전문적인 데이터 분석 도구나 소프트웨어 (예: Excel, Google Sheets, Tableau, Python/R 등)를 활용하시는 것이 좋습니다. 만약 특정 분석 기법이나 도구 사용법에 대해 자세한 조언이 필요하시다면 알려주세요!\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "질문: 홍길동의 전화번호와 주소를 알려줄래?\n",
      "\n",
      "로컬 모델 사용 가능: True\n",
      "질문 민감도: high\n",
      "\n",
      "선택된 모델 타입: cloud\n",
      "모델 이름: gpt-4o-mini\n",
      "선택 이유: 높은 성능이 필요한 일반 질문\n",
      "모델 장점: 높은 성능, 안정적인 서비스, 복잡한 추론 가능\n",
      "\n",
      "답변:\n",
      "죄송합니다, 하지만 홍길동이라는 이름의 개인에 대한 실제 연락처 정보를 공개하거나 제공하는 것은 적절하지 않습니다. 개인 정보 보호를 위해 특정 인물의 전화번호나 주소는 공유되지 않습니다. 만약 관련 정보가 필요하시다면, 공식적인 채널이나 직접적인 연락 방법을 통해 확인하시는 것이 가장 좋습니다. 어떤 맥락에서 이 정보가 필요하신지 좀 더 자세히 알려주시면 도움을 드릴 수 있는 방법을 찾아볼 수 있을 것 같습니다.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 실행\n",
    "test_queries = [\n",
    "    \"Python에서 리스트와 튜플의 차이는?\",\n",
    "    \"우리 회사의 매출 데이터를 분석해줘\",\n",
    "    \"홍길동의 전화번호와 주소를 알려줄래?\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for query in test_queries:\n",
    "    print(\"=\" * 80)\n",
    "    result = llm_serving_agent(query)\n",
    "    results.append(result)\n",
    "    print(\"=\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 모델 사용 통계 =====\n",
      "총 질문 수: 3건\n",
      "\n",
      "모델 타입별 사용 횟수:\n",
      "  - cloud: 3건 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# 결과 통계\n",
    "print(\"\\n===== 모델 사용 통계 =====\")\n",
    "model_usage = {}\n",
    "\n",
    "for result in results:\n",
    "    model_type = result[\"model_type\"]\n",
    "    model_usage[model_type] = model_usage.get(model_type, 0) + 1\n",
    "\n",
    "print(f\"총 질문 수: {len(results)}건\\n\")\n",
    "print(\"모델 타입별 사용 횟수:\")\n",
    "for model_type, count in model_usage.items():\n",
    "    percentage = (count / len(results)) * 100\n",
    "    print(f\"  - {model_type}: {count}건 ({percentage:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
