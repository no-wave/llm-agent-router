{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Router LLM Model Agent - 커피 키오스크 주문 시스템"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env 파일에서 환경 변수를 로드한다\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "%pip install -q pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Optional, Literal\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "# OpenAI 클라이언트 초기화\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pydantic 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v9/46y9d8bn1lxgjt7g439hsf8c0000gn/T/ipykernel_94615/1450928007.py:64: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator('overall_score', always=True)\n"
     ]
    }
   ],
   "source": [
    "class QueryComplexity(BaseModel):\n",
    "    \"\"\"사용자 질문의 복잡도를 분석한 결과를 표현하는 모델\"\"\"\n",
    "    complexity_level: Literal[\"simple\", \"moderate\", \"complex\"] = Field(\n",
    "        description=\"질문의 복잡도: simple(단순), moderate(보통), complex(복잡)\"\n",
    "    )\n",
    "    reasoning_depth: int = Field(\n",
    "        ge=1, le=5,\n",
    "        description=\"필요한 추론 깊이 (1: 단순 조회, 5: 다단계 추론)\"\n",
    "    )\n",
    "    response_time_priority: Literal[\"fast\", \"balanced\", \"quality\"] = Field(\n",
    "        description=\"응답 시간 우선순위\"\n",
    "    )\n",
    "    requires_creativity: bool = Field(\n",
    "        description=\"창의적 답변이 필요한지 여부\"\n",
    "    )\n",
    "    analysis_reason: str = Field(\n",
    "        description=\"복잡도 분석 근거\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ModelSpecification(BaseModel):\n",
    "    \"\"\"LLM 모델의 사양과 특성을 표현하는 모델\"\"\"\n",
    "    model_name: str = Field(description=\"모델 이름\")\n",
    "    model_tier: Literal[\"nano\", \"mini\", \"standard\"] = Field(\n",
    "        description=\"모델 계층\"\n",
    "    )\n",
    "    max_completion_tokens: int = Field(description=\"최대 토큰 수\")\n",
    "    cost_per_1k_tokens: float = Field(description=\"1K 토큰당 비용 (USD)\")\n",
    "    avg_latency_ms: int = Field(description=\"평균 응답 지연시간 (밀리초)\")\n",
    "    suitable_for: List[str] = Field(description=\"적합한 태스크 유형\")\n",
    "\n",
    "\n",
    "class ModelSelection(BaseModel):\n",
    "    \"\"\"모델 선택 결과를 표현하는 모델\"\"\"\n",
    "    selected_model: ModelSpecification = Field(description=\"선택된 모델\")\n",
    "    confidence: float = Field(\n",
    "        ge=0.0, le=1.0,\n",
    "        description=\"모델 선택에 대한 신뢰도\"\n",
    "    )\n",
    "    selection_reason: str = Field(description=\"모델 선택 이유\")\n",
    "    alternative_models: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"대안 모델 목록\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ConversationMessage(BaseModel):\n",
    "    \"\"\"대화 메시지를 표현하는 모델\"\"\"\n",
    "    role: Literal[\"user\", \"assistant\", \"system\"] = Field(description=\"메시지 역할\")\n",
    "    content: str = Field(description=\"메시지 내용\")\n",
    "    timestamp: datetime = Field(default_factory=datetime.now, description=\"메시지 생성 시간\")\n",
    "    model_used: Optional[str] = Field(default=None, description=\"사용된 모델\")\n",
    "    metadata: Dict = Field(default_factory=dict, description=\"추가 메타데이터\")\n",
    "\n",
    "\n",
    "class ResponseQuality(BaseModel):\n",
    "    \"\"\"생성된 응답의 품질을 표현하는 모델\"\"\"\n",
    "    accuracy: float = Field(ge=0.0, le=1.0, description=\"정확도\")\n",
    "    completeness: float = Field(ge=0.0, le=1.0, description=\"완전성\")\n",
    "    clarity: float = Field(ge=0.0, le=1.0, description=\"명확성\")\n",
    "    relevance: float = Field(ge=0.0, le=1.0, description=\"관련성\")\n",
    "    overall_score: float = Field(ge=0.0, le=1.0, description=\"종합 점수\")\n",
    "    \n",
    "    @validator('overall_score', always=True)\n",
    "    def calculate_overall_score(cls, v, values):\n",
    "        \"\"\"개별 점수들의 평균으로 종합 점수를 계산한다\"\"\"\n",
    "        if all(k in values for k in ['accuracy', 'completeness', 'clarity', 'relevance']):\n",
    "            return (values['accuracy'] + values['completeness'] + \n",
    "                   values['clarity'] + values['relevance']) / 4\n",
    "        return v\n",
    "\n",
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    \"\"\"에이전트의 최종 응답을 표현하는 모델\"\"\"\n",
    "    answer: str = Field(description=\"사용자에게 제공되는 답변\")\n",
    "    model_used: str = Field(description=\"사용된 모델\")\n",
    "    complexity_analysis: QueryComplexity = Field(description=\"질문 복잡도 분석\")\n",
    "    response_time_ms: int = Field(description=\"응답 생성 시간 (밀리초)\")\n",
    "    quality_score: ResponseQuality = Field(description=\"응답 품질 점수\")\n",
    "    was_escalated: bool = Field(description=\"상위 모델로 에스컬레이션 되었는지 여부\")\n",
    "    total_cost_usd: float = Field(description=\"총 비용 (USD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 레지스트리 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 3개의 모델이 등록되었다.\n",
      "  - gpt-5-nano (nano): 인사, 단순 확인...\n",
      "  - gpt-5-mini (mini): 일반 주문, 메뉴 추천...\n",
      "  - gpt-4o-mini (standard): 복잡한 조합, 맞춤형 추천...\n"
     ]
    }
   ],
   "source": [
    "# 모델 사양 정의 (실제 운영 환경의 값을 시뮬레이션)\n",
    "MODEL_REGISTRY = {\n",
    "    \"gpt-5-nano\": ModelSpecification(\n",
    "        model_name=\"gpt-5-nano\",  # nano는 mini의 낮은 temperature로 시뮬레이션\n",
    "        model_tier=\"nano\",\n",
    "        max_completion_tokens=500,\n",
    "        cost_per_1k_tokens=0.00015,\n",
    "        avg_latency_ms=200,\n",
    "        suitable_for=[\"인사\", \"단순 확인\", \"메뉴 조회\", \"가격 문의\"]\n",
    "    ),\n",
    "    \"gpt-5-mini\": ModelSpecification(\n",
    "        model_name=\"gpt-5-mini\",\n",
    "        model_tier=\"mini\",\n",
    "        max_completion_tokens=2000,\n",
    "        cost_per_1k_tokens=0.00030,\n",
    "        avg_latency_ms=500,\n",
    "        suitable_for=[\"일반 주문\", \"메뉴 추천\", \"옵션 설명\", \"간단한 조합\"]\n",
    "    ),\n",
    "    \"gpt-4o-mini\": ModelSpecification(\n",
    "        model_name=\"gpt-4o-mini\",  # 실제로는 gpt-4o를 사용하지만 여기서는 mini로 시뮬레이션\n",
    "        model_tier=\"standard\",\n",
    "        max_completion_tokens=4000,\n",
    "        cost_per_1k_tokens=0.00500,\n",
    "        avg_latency_ms=1500,\n",
    "        suitable_for=[\"복잡한 조합\", \"맞춤형 추천\", \"다단계 추론\", \"창의적 제안\"]\n",
    "    )\n",
    "}\n",
    "\n",
    "# 모델 계층 순서 (에스컬레이션용)\n",
    "MODEL_HIERARCHY = [\"gpt-5-nano\", \"gpt-5-mini\", \"gpt-4o-mini\"]\n",
    "\n",
    "print(f\"총 {len(MODEL_REGISTRY)}개의 모델이 등록되었다.\")\n",
    "for model_id, spec in MODEL_REGISTRY.items():\n",
    "    print(f\"  - {model_id} ({spec.model_tier}): {', '.join(spec.suitable_for[:2])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory 시스템 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대화 메모리가 초기화되었다.\n"
     ]
    }
   ],
   "source": [
    "class ConversationMemory:\n",
    "    \"\"\"대화 이력과 모델 사용 이력을 관리하는 메모리 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, max_history: int = 10):\n",
    "        self.messages: List[ConversationMessage] = []\n",
    "        self.max_history = max_history\n",
    "        self.model_usage_stats: Dict[str, Dict] = {}\n",
    "        \n",
    "        # 각 모델의 사용 통계 초기화\n",
    "        for model_id in MODEL_REGISTRY.keys():\n",
    "            self.model_usage_stats[model_id] = {\n",
    "                \"count\": 0,\n",
    "                \"total_quality\": 0.0,\n",
    "                \"success_count\": 0\n",
    "            }\n",
    "    \n",
    "    def add_message(self, role: str, content: str, model_used: str = None, metadata: Dict = None):\n",
    "        \"\"\"새로운 메시지를 메모리에 추가한다\"\"\"\n",
    "        message = ConversationMessage(\n",
    "            role=role,\n",
    "            content=content,\n",
    "            model_used=model_used,\n",
    "            metadata=metadata or {}\n",
    "        )\n",
    "        self.messages.append(message)\n",
    "        \n",
    "        # 최대 이력 수를 초과하면 오래된 메시지부터 제거한다\n",
    "        if len(self.messages) > self.max_history:\n",
    "            self.messages = self.messages[-self.max_history:]\n",
    "    \n",
    "    def update_model_stats(self, model_id: str, quality_score: float, success: bool):\n",
    "        \"\"\"모델 사용 통계를 업데이트한다\"\"\"\n",
    "        if model_id in self.model_usage_stats:\n",
    "            stats = self.model_usage_stats[model_id]\n",
    "            stats[\"count\"] += 1\n",
    "            stats[\"total_quality\"] += quality_score\n",
    "            if success:\n",
    "                stats[\"success_count\"] += 1\n",
    "    \n",
    "    def get_model_performance(self, model_id: str) -> Dict:\n",
    "        \"\"\"특정 모델의 성능 통계를 반환한다\"\"\"\n",
    "        if model_id not in self.model_usage_stats:\n",
    "            return None\n",
    "        \n",
    "        stats = self.model_usage_stats[model_id]\n",
    "        if stats[\"count\"] == 0:\n",
    "            return {\n",
    "                \"avg_quality\": 0.0,\n",
    "                \"success_rate\": 0.0,\n",
    "                \"usage_count\": 0\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"avg_quality\": stats[\"total_quality\"] / stats[\"count\"],\n",
    "            \"success_rate\": stats[\"success_count\"] / stats[\"count\"],\n",
    "            \"usage_count\": stats[\"count\"]\n",
    "        }\n",
    "    \n",
    "    def get_context(self, include_system: bool = True) -> List[Dict]:\n",
    "        \"\"\"OpenAI API 형식으로 대화 이력을 반환한다\"\"\"\n",
    "        context = []\n",
    "        for msg in self.messages:\n",
    "            if msg.role == \"system\" and not include_system:\n",
    "                continue\n",
    "            context.append({\n",
    "                \"role\": msg.role,\n",
    "                \"content\": msg.content\n",
    "            })\n",
    "        return context\n",
    "    \n",
    "    def get_summary(self) -> str:\n",
    "        \"\"\"메모리 상태 요약을 반환한다\"\"\"\n",
    "        total_messages = len(self.messages)\n",
    "        total_model_uses = sum(stats[\"count\"] for stats in self.model_usage_stats.values())\n",
    "        return f\"메시지: {total_messages}개, 모델 호출: {total_model_uses}회\"\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"모든 이력을 삭제한다\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "# 메모리 인스턴스 생성\n",
    "memory = ConversationMemory(max_history=10)\n",
    "\n",
    "# 시스템 메시지 추가\n",
    "memory.add_message(\n",
    "    \"system\",\n",
    "    \"당신은 커피 키오스크의 친절한 주문 도우미다. 고객의 질문에 정확하고 친절하게 답변하라.\"\n",
    ")\n",
    "\n",
    "print(\"대화 메모리가 초기화되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 복잡도 분석 시스템 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "복잡도 분석 시스템이 초기화되었다.\n"
     ]
    }
   ],
   "source": [
    "class ComplexityAnalyzer:\n",
    "    \"\"\"질문 복잡도 분석 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 복잡도 분석용 경량 모델 사용\n",
    "        self.analyzer_model = \"gpt-4o-mini\"\n",
    "    \n",
    "    def analyze(self, query: str, context: List[Dict] = None) -> QueryComplexity:\n",
    "        \"\"\"\n",
    "        사용자 질문의 복잡도를 분석한다\n",
    "        \n",
    "        Args:\n",
    "            query: 사용자 질문\n",
    "            context: 대화 컨텍스트 (선택적)\n",
    "        \n",
    "        Returns:\n",
    "            QueryComplexity 객체\n",
    "        \"\"\"\n",
    "        analysis_prompt = f\"\"\"\n",
    "다음 커피 키오스크 고객 질문의 복잡도를 분석하라.\n",
    "\n",
    "질문: \"{query}\"\n",
    "\n",
    "다음 기준으로 평가하라:\n",
    "\n",
    "1. 복잡도 수준 (complexity_level):\n",
    "   - simple: 단순 인사, 예/아니오 질문, 단일 메뉴 조회\n",
    "   - moderate: 메뉴 비교, 옵션 설명, 일반적인 추천\n",
    "   - complex: 다중 조건 추천, 복잡한 조합, 상황별 맞춤 제안\n",
    "\n",
    "2. 추론 깊이 (reasoning_depth): 1-5\n",
    "   - 1: 단순 정보 조회\n",
    "   - 3: 비교 및 추천\n",
    "   - 5: 다단계 추론 및 창의적 해결\n",
    "\n",
    "3. 응답 시간 우선순위 (response_time_priority):\n",
    "   - fast: 즉각 응답 필요\n",
    "   - balanced: 균형잡힌 속도와 품질\n",
    "   - quality: 품질 최우선\n",
    "\n",
    "4. 창의성 필요 (requires_creativity): true/false\n",
    "\n",
    "5. 분석 근거 (analysis_reason): 한 문장으로 설명\n",
    "\n",
    "JSON 형식으로만 답변하라:\n",
    "{{\n",
    "  \"complexity_level\": \"simple|moderate|complex\",\n",
    "  \"reasoning_depth\": 1-5,\n",
    "  \"response_time_priority\": \"fast|balanced|quality\",\n",
    "  \"requires_creativity\": true|false,\n",
    "  \"analysis_reason\": \"설명\"\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": analysis_prompt}]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=self.analyzer_model,\n",
    "            messages=messages,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        \n",
    "        return QueryComplexity(**result)\n",
    "\n",
    "# 복잡도 분석기 인스턴스 생성\n",
    "complexity_analyzer = ComplexityAnalyzer()\n",
    "print(\"복잡도 분석 시스템이 초기화되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Function Calling & Tool 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 3개의 모델 선택 도구가 정의되었다.\n"
     ]
    }
   ],
   "source": [
    "# 모델 선택을 위한 도구 정의\n",
    "MODEL_SELECTION_TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"select_nano_model\",\n",
    "            \"description\": \"경량 모델을 선택한다. 단순한 질문이나 빠른 응답이 필요한 경우 사용한다.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"reason\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"이 모델을 선택한 이유\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"reason\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"select_mini_model\",\n",
    "            \"description\": \"표준 모델을 선택한다. 일반적인 주문이나 메뉴 추천에 사용한다.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"reason\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"이 모델을 선택한 이유\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"reason\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"select_standard_model\",\n",
    "            \"description\": \"고성능 모델을 선택한다. 복잡한 추론이나 창의적 답변이 필요한 경우 사용한다.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"reason\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"이 모델을 선택한 이유\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"reason\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# 도구 이름과 모델 ID 매핑\n",
    "TOOL_TO_MODEL_MAP = {\n",
    "    \"select_nano_model\": \"gpt-5-nano\",\n",
    "    \"select_mini_model\": \"gpt-5-mini\",\n",
    "    \"select_standard_model\": \"gpt-4o-mini\"\n",
    "}\n",
    "\n",
    "print(f\"총 {len(MODEL_SELECTION_TOOLS)}개의 모델 선택 도구가 정의되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델 선택 라우터 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 라우터가 초기화되었다.\n"
     ]
    }
   ],
   "source": [
    "class ModelRouter:\n",
    "    \"\"\"모델 선택 라우터 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: ConversationMemory):\n",
    "        self.memory = memory\n",
    "        self.router_model = \"gpt-4o-mini\"\n",
    "    \n",
    "    def route(self, query: str, complexity: QueryComplexity) -> ModelSelection:\n",
    "        \"\"\"\n",
    "        질문과 복잡도 분석 결과를 바탕으로 최적의 모델을 선택한다\n",
    "        \n",
    "        Args:\n",
    "            query: 사용자 질문\n",
    "            complexity: 복잡도 분석 결과\n",
    "        \n",
    "        Returns:\n",
    "            ModelSelection 객체\n",
    "        \"\"\"\n",
    "        # 복잡도 기반 초기 모델 추천\n",
    "        routing_prompt = f\"\"\"\n",
    "커피 키오스크 질문에 대해 최적의 LLM 모델을 선택하라.\n",
    "\n",
    "질문: \"{query}\"\n",
    "\n",
    "복잡도 분석:\n",
    "- 복잡도: {complexity.complexity_level}\n",
    "- 추론 깊이: {complexity.reasoning_depth}\n",
    "- 응답 시간 우선순위: {complexity.response_time_priority}\n",
    "- 창의성 필요: {complexity.requires_creativity}\n",
    "- 분석 근거: {complexity.analysis_reason}\n",
    "\n",
    "사용 가능한 모델:\n",
    "1. gpt-5-nano: 경량 모델 (빠름, 저비용) - {', '.join(MODEL_REGISTRY['gpt-5-nano'].suitable_for)}\n",
    "2. gpt-5-mini: 표준 모델 (균형) - {', '.join(MODEL_REGISTRY['gpt-5-mini'].suitable_for)}\n",
    "3. gpt-4o-mini: 고성능 모델 (느림, 고비용) - {', '.join(MODEL_REGISTRY['gpt-4o-mini'].suitable_for)}\n",
    "\n",
    "과거 모델 성능:\n",
    "\"\"\"\n",
    "        \n",
    "        # 모델별 과거 성능 정보 추가\n",
    "        for model_id in MODEL_HIERARCHY:\n",
    "            perf = self.memory.get_model_performance(model_id)\n",
    "            routing_prompt += f\"\\n- {model_id}: 평균 품질 {perf['avg_quality']:.2f}, 성공률 {perf['success_rate']:.1%}\"\n",
    "        \n",
    "        routing_prompt += \"\\n\\n가장 적합한 모델을 선택하는 함수를 호출하라.\"\n",
    "        \n",
    "        # Function Calling으로 모델 선택\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.router_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": routing_prompt}],\n",
    "            tools=MODEL_SELECTION_TOOLS,\n",
    "            tool_choice=\"required\"\n",
    "        )\n",
    "        \n",
    "        # 도구 호출 결과 파싱\n",
    "        tool_call = response.choices[0].message.tool_calls[0]\n",
    "        function_name = tool_call.function.name\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        # 선택된 모델 ID 가져오기\n",
    "        selected_model_id = TOOL_TO_MODEL_MAP[function_name]\n",
    "        selected_spec = MODEL_REGISTRY[selected_model_id]\n",
    "        \n",
    "        # 대안 모델 목록 생성\n",
    "        alternatives = [m for m in MODEL_HIERARCHY if m != selected_model_id]\n",
    "        \n",
    "        # 신뢰도 계산 (복잡도와 모델 tier의 적합성 기반)\n",
    "        confidence = self._calculate_selection_confidence(complexity, selected_spec)\n",
    "        \n",
    "        return ModelSelection(\n",
    "            selected_model=selected_spec,\n",
    "            confidence=confidence,\n",
    "            selection_reason=function_args[\"reason\"],\n",
    "            alternative_models=alternatives\n",
    "        )\n",
    "    \n",
    "    def _calculate_selection_confidence(self, complexity: QueryComplexity, model: ModelSpecification) -> float:\n",
    "        \"\"\"\n",
    "        복잡도와 모델의 적합성을 기반으로 선택 신뢰도를 계산한다\n",
    "        \n",
    "        Returns:\n",
    "            0.0 ~ 1.0 사이의 신뢰도 점수\n",
    "        \"\"\"\n",
    "        # 복잡도와 모델 tier의 매칭도\n",
    "        tier_match = {\n",
    "            (\"simple\", \"nano\"): 0.95,\n",
    "            (\"simple\", \"mini\"): 0.70,\n",
    "            (\"simple\", \"standard\"): 0.50,\n",
    "            (\"moderate\", \"nano\"): 0.60,\n",
    "            (\"moderate\", \"mini\"): 0.95,\n",
    "            (\"moderate\", \"standard\"): 0.75,\n",
    "            (\"complex\", \"nano\"): 0.30,\n",
    "            (\"complex\", \"mini\"): 0.70,\n",
    "            (\"complex\", \"standard\"): 0.95,\n",
    "        }\n",
    "        \n",
    "        base_confidence = tier_match.get((complexity.complexity_level, model.model_tier), 0.5)\n",
    "        \n",
    "        # 창의성 요구와 모델 tier 고려\n",
    "        if complexity.requires_creativity and model.model_tier in [\"mini\", \"standard\"]:\n",
    "            base_confidence += 0.05\n",
    "        \n",
    "        return min(base_confidence, 1.0)\n",
    "\n",
    "# 모델 라우터 인스턴스 생성\n",
    "model_router = ModelRouter(memory)\n",
    "print(\"모델 라우터가 초기화되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validation 시스템 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 시스템이 초기화되었다.\n"
     ]
    }
   ],
   "source": [
    "class ValidationSystem:\n",
    "    \"\"\"모델 선택 및 응답 품질 검증 시스템 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, min_confidence: float = 0.6, min_quality: float = 0.7):\n",
    "        self.min_confidence = min_confidence\n",
    "        self.min_quality = min_quality\n",
    "        self.validator_model = \"gpt-4o-mini\"\n",
    "    \n",
    "    def validate_model_selection(self, selection: ModelSelection, complexity: QueryComplexity) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        모델 선택의 적합성을 검증한다\n",
    "        \n",
    "        Returns:\n",
    "            (검증 통과 여부, 검증 메시지)\n",
    "        \"\"\"\n",
    "        # 신뢰도 확인\n",
    "        if selection.confidence < self.min_confidence:\n",
    "            return False, f\"모델 선택 신뢰도가 낮다 ({selection.confidence:.2f} < {self.min_confidence})\"\n",
    "        \n",
    "        # 복잡도와 모델의 불일치 확인\n",
    "        if complexity.complexity_level == \"complex\" and selection.selected_model.model_tier == \"nano\":\n",
    "            return False, \"복잡한 질문에 경량 모델이 선택되었다\"\n",
    "        \n",
    "        if complexity.complexity_level == \"simple\" and selection.selected_model.model_tier == \"standard\":\n",
    "            return False, \"단순한 질문에 고성능 모델이 선택되었다 (비효율적)\"\n",
    "        \n",
    "        return True, \"모델 선택이 적합하다\"\n",
    "    \n",
    "    def validate_response(self, query: str, answer: str, model_used: str) -> ResponseQuality:\n",
    "        \"\"\"\n",
    "        생성된 응답의 품질을 평가한다\n",
    "        \n",
    "        Args:\n",
    "            query: 사용자 질문\n",
    "            answer: 생성된 답변\n",
    "            model_used: 사용된 모델\n",
    "        \n",
    "        Returns:\n",
    "            ResponseQuality 객체\n",
    "        \"\"\"\n",
    "        validation_prompt = f\"\"\"\n",
    "커피 키오스크 질문과 답변의 품질을 평가하라.\n",
    "\n",
    "질문: {query}\n",
    "답변: {answer}\n",
    "사용 모델: {model_used}\n",
    "\n",
    "다음 기준으로 0.0-1.0 점수를 매겨라:\n",
    "1. accuracy: 답변의 정확성 (사실 관계가 올바른가?)\n",
    "2. completeness: 답변의 완전성 (질문에 충분히 답했는가?)\n",
    "3. clarity: 답변의 명확성 (이해하기 쉬운가?)\n",
    "4. relevance: 답변의 관련성 (질문과 관련있는가?)\n",
    "\n",
    "JSON 형식으로만 답변하라:\n",
    "{{\n",
    "  \"accuracy\": 0.0-1.0,\n",
    "  \"completeness\": 0.0-1.0,\n",
    "  \"clarity\": 0.0-1.0,\n",
    "  \"relevance\": 0.0-1.0,\n",
    "  \"overall_score\": 0.0-1.0\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=self.validator_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": validation_prompt}],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return ResponseQuality(**result)\n",
    "    \n",
    "    def should_escalate(self, quality: ResponseQuality) -> bool:\n",
    "        \"\"\"\n",
    "        응답 품질이 낮아 상위 모델로 에스컬레이션해야 하는지 판단한다\n",
    "        \n",
    "        Returns:\n",
    "            에스컬레이션 필요 여부\n",
    "        \"\"\"\n",
    "        return quality.overall_score < self.min_quality\n",
    "\n",
    "# 검증 시스템 인스턴스 생성\n",
    "validator = ValidationSystem(min_confidence=0.6, min_quality=0.7)\n",
    "print(\"검증 시스템이 초기화되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Recovery 메커니즘 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "복구 시스템이 초기화되었다.\n"
     ]
    }
   ],
   "source": [
    "class RecoverySystem:\n",
    "    \"\"\"복구 및 에스컬레이션 메커니즘 시스템 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.escalation_history = []\n",
    "        self.max_escalations = 2\n",
    "    \n",
    "    def get_next_model(self, current_model_id: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        현재 모델보다 상위 모델을 반환한다\n",
    "        \n",
    "        Args:\n",
    "            current_model_id: 현재 모델 ID\n",
    "        \n",
    "        Returns:\n",
    "            다음 모델 ID 또는 None (최상위 모델인 경우)\n",
    "        \"\"\"\n",
    "        current_idx = MODEL_HIERARCHY.index(current_model_id)\n",
    "        \n",
    "        if current_idx >= len(MODEL_HIERARCHY) - 1:\n",
    "            return None\n",
    "        \n",
    "        return MODEL_HIERARCHY[current_idx + 1]\n",
    "    \n",
    "    def attempt_escalation(\n",
    "        self,\n",
    "        query: str,\n",
    "        current_model_id: str,\n",
    "        failure_reason: str\n",
    "    ) -> tuple[Optional[str], str]:\n",
    "        \"\"\"\n",
    "        상위 모델로 에스컬레이션을 시도한다\n",
    "        \n",
    "        Args:\n",
    "            query: 사용자 질문\n",
    "            current_model_id: 현재 실패한 모델 ID\n",
    "            failure_reason: 실패 이유\n",
    "        \n",
    "        Returns:\n",
    "            (다음 모델 ID, 메시지)\n",
    "        \"\"\"\n",
    "        # 에스컬레이션 이력 확인\n",
    "        if len(self.escalation_history) >= self.max_escalations:\n",
    "            return None, \"최대 에스컬레이션 횟수에 도달했다\"\n",
    "        \n",
    "        # 다음 모델 가져오기\n",
    "        next_model_id = self.get_next_model(current_model_id)\n",
    "        \n",
    "        if next_model_id is None:\n",
    "            return None, \"이미 최상위 모델이다\"\n",
    "        \n",
    "        # 에스컬레이션 기록\n",
    "        self.escalation_history.append({\n",
    "            \"from_model\": current_model_id,\n",
    "            \"to_model\": next_model_id,\n",
    "            \"reason\": failure_reason,\n",
    "            \"timestamp\": datetime.now()\n",
    "        })\n",
    "        \n",
    "        print(f\"Recovery: {current_model_id} → {next_model_id} 에스컬레이션 ({failure_reason})\")\n",
    "        \n",
    "        return next_model_id, f\"{current_model_id}에서 {next_model_id}로 에스컬레이션되었다\"\n",
    "    \n",
    "    def generate_fallback_response(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        모든 복구 시도가 실패했을 때 대체 응답을 생성한다\n",
    "        \n",
    "        Args:\n",
    "            query: 원본 질문\n",
    "        \n",
    "        Returns:\n",
    "            대체 답변\n",
    "        \"\"\"\n",
    "        return f\"죄송하지만 '{query}'에 대한 적절한 답변을 생성하지 못했다. 직원에게 직접 문의해 주시기 바란다.\"\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"에스컬레이션 이력을 초기화한다\"\"\"\n",
    "        self.escalation_history = []\n",
    "    \n",
    "    def get_escalation_stats(self) -> Dict:\n",
    "        \"\"\"에스컬레이션 통계를 반환한다\"\"\"\n",
    "        if not self.escalation_history:\n",
    "            return {\"count\": 0, \"most_common_reason\": None}\n",
    "        \n",
    "        reasons = [e[\"reason\"] for e in self.escalation_history]\n",
    "        most_common = max(set(reasons), key=reasons.count) if reasons else None\n",
    "        \n",
    "        return {\n",
    "            \"count\": len(self.escalation_history),\n",
    "            \"most_common_reason\": most_common\n",
    "        }\n",
    "\n",
    "# 복구 시스템 인스턴스 생성\n",
    "recovery_system = RecoverySystem()\n",
    "print(\"복구 시스템이 초기화되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feedback 시스템 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피드백 시스템이 초기화되었다.\n"
     ]
    }
   ],
   "source": [
    "class FeedbackSystem:\n",
    "    \"\"\"피드백 및 학습 시스템 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feedback_records = []\n",
    "    \n",
    "    def record_interaction(\n",
    "        self,\n",
    "        query: str,\n",
    "        complexity: QueryComplexity,\n",
    "        model_used: str,\n",
    "        answer: str,\n",
    "        quality: ResponseQuality,\n",
    "        was_escalated: bool\n",
    "    ):\n",
    "        \"\"\"\n",
    "        상호작용 기록을 저장한다\n",
    "        \n",
    "        Args:\n",
    "            query: 사용자 질문\n",
    "            complexity: 복잡도 분석 결과\n",
    "            model_used: 사용된 모델\n",
    "            answer: 생성된 답변\n",
    "            quality: 품질 평가 결과\n",
    "            was_escalated: 에스컬레이션 여부\n",
    "        \"\"\"\n",
    "        record = {\n",
    "            \"query\": query,\n",
    "            \"complexity_level\": complexity.complexity_level,\n",
    "            \"model_used\": model_used,\n",
    "            \"quality_score\": quality.overall_score,\n",
    "            \"was_escalated\": was_escalated,\n",
    "            \"timestamp\": datetime.now()\n",
    "        }\n",
    "        self.feedback_records.append(record)\n",
    "    \n",
    "    def analyze_model_performance(self) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        각 모델의 성능을 분석한다\n",
    "        \n",
    "        Returns:\n",
    "            모델별 성능 통계\n",
    "        \"\"\"\n",
    "        performance = {}\n",
    "        \n",
    "        for model_id in MODEL_HIERARCHY:\n",
    "            model_records = [r for r in self.feedback_records if r[\"model_used\"] == model_id]\n",
    "            \n",
    "            if not model_records:\n",
    "                performance[model_id] = {\n",
    "                    \"usage_count\": 0,\n",
    "                    \"avg_quality\": 0.0,\n",
    "                    \"escalation_rate\": 0.0\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            avg_quality = sum(r[\"quality_score\"] for r in model_records) / len(model_records)\n",
    "            escalation_rate = sum(r[\"was_escalated\"] for r in model_records) / len(model_records)\n",
    "            \n",
    "            performance[model_id] = {\n",
    "                \"usage_count\": len(model_records),\n",
    "                \"avg_quality\": avg_quality,\n",
    "                \"escalation_rate\": escalation_rate\n",
    "            }\n",
    "        \n",
    "        return performance\n",
    "    \n",
    "    def get_optimization_suggestions(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        성능 분석을 바탕으로 최적화 제안을 생성한다\n",
    "        \n",
    "        Returns:\n",
    "            최적화 제안 리스트\n",
    "        \"\"\"\n",
    "        if not self.feedback_records:\n",
    "            return [\"데이터가 충분하지 않다.\"]\n",
    "        \n",
    "        suggestions = []\n",
    "        performance = self.analyze_model_performance()\n",
    "        \n",
    "        # 각 모델별 분석\n",
    "        for model_id, stats in performance.items():\n",
    "            if stats[\"usage_count\"] == 0:\n",
    "                continue\n",
    "            \n",
    "            # 에스컬레이션 비율이 높은 경우\n",
    "            if stats[\"escalation_rate\"] > 0.3:\n",
    "                suggestions.append(\n",
    "                    f\"{model_id}의 에스컬레이션 비율이 {stats['escalation_rate']:.1%}로 높다. \"\n",
    "                    f\"이 모델의 사용 기준을 재검토하라.\"\n",
    "                )\n",
    "            \n",
    "            # 품질이 낮은 경우\n",
    "            if stats[\"avg_quality\"] < 0.7:\n",
    "                suggestions.append(\n",
    "                    f\"{model_id}의 평균 품질이 {stats['avg_quality']:.2f}로 낮다. \"\n",
    "                    f\"더 높은 tier의 모델 사용을 고려하라.\"\n",
    "                )\n",
    "        \n",
    "        # 전체 에스컬레이션 비율\n",
    "        total_escalations = sum(r[\"was_escalated\"] for r in self.feedback_records)\n",
    "        escalation_rate = total_escalations / len(self.feedback_records)\n",
    "        \n",
    "        if escalation_rate > 0.2:\n",
    "            suggestions.append(\n",
    "                f\"전체 에스컬레이션 비율이 {escalation_rate:.1%}다. \"\n",
    "                f\"초기 모델 선택 기준을 상향 조정하는 것을 고려하라.\"\n",
    "            )\n",
    "        \n",
    "        return suggestions if suggestions else [\"현재 모델 선택 전략이 잘 작동하고 있다.\"]\n",
    "    \n",
    "    def get_cost_analysis(self) -> Dict:\n",
    "        \"\"\"\n",
    "        비용 분석을 수행한다\n",
    "        \n",
    "        Returns:\n",
    "            비용 분석 결과\n",
    "        \"\"\"\n",
    "        if not self.feedback_records:\n",
    "            return {\"total_cost\": 0.0, \"avg_cost_per_query\": 0.0}\n",
    "        \n",
    "        # 간단한 비용 추정 (실제로는 토큰 수를 계산해야 함)\n",
    "        total_cost = 0.0\n",
    "        for record in self.feedback_records:\n",
    "            model_spec = MODEL_REGISTRY[record[\"model_used\"]]\n",
    "            # 평균 500 토큰 사용으로 가정\n",
    "            estimated_cost = model_spec.cost_per_1k_tokens * 0.5\n",
    "            total_cost += estimated_cost\n",
    "        \n",
    "        return {\n",
    "            \"total_cost\": total_cost,\n",
    "            \"avg_cost_per_query\": total_cost / len(self.feedback_records),\n",
    "            \"query_count\": len(self.feedback_records)\n",
    "        }\n",
    "\n",
    "# 피드백 시스템 인스턴스 생성\n",
    "feedback_system = FeedbackSystem()\n",
    "print(\"피드백 시스템이 초기화되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Router LLM Model Agent 통합 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Router LLM Model Agent가 초기화되었다.\n"
     ]
    }
   ],
   "source": [
    "class RouterLLMModelAgent:\n",
    "    \"\"\"Router LLM Model Agent 메인 클래스\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        complexity_analyzer: ComplexityAnalyzer,\n",
    "        model_router: ModelRouter,\n",
    "        memory: ConversationMemory,\n",
    "        validator: ValidationSystem,\n",
    "        recovery: RecoverySystem,\n",
    "        feedback: FeedbackSystem\n",
    "    ):\n",
    "        self.complexity_analyzer = complexity_analyzer\n",
    "        self.model_router = model_router\n",
    "        self.memory = memory\n",
    "        self.validator = validator\n",
    "        self.recovery = recovery\n",
    "        self.feedback = feedback\n",
    "    \n",
    "    def generate_response(self, query: str, model_id: str) -> tuple[str, int]:\n",
    "        \"\"\"\n",
    "        선택된 모델로 응답을 생성한다\n",
    "        \n",
    "        Args:\n",
    "            query: 사용자 질문\n",
    "            model_id: 사용할 모델 ID\n",
    "        \n",
    "        Returns:\n",
    "            (생성된 답변, 응답 시간 ms)\n",
    "        \"\"\"\n",
    "        model_spec = MODEL_REGISTRY[model_id]\n",
    "        \n",
    "        # 대화 컨텍스트 가져오기\n",
    "        messages = self.memory.get_context()\n",
    "        messages.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # 응답 생성 시간 측정\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model_spec.model_name,\n",
    "            messages=messages,\n",
    "            max_completion_tokens=model_spec.max_completion_tokens,\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time_ms = int((end_time - start_time) * 1000)\n",
    "        \n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        \n",
    "        return answer, response_time_ms\n",
    "    \n",
    "    def process_query(self, user_query: str) -> AgentResponse:\n",
    "        \"\"\"\n",
    "        사용자 질문을 처리하고 최종 응답을 생성한다\n",
    "        \n",
    "        Args:\n",
    "            user_query: 사용자 질문\n",
    "        \n",
    "        Returns:\n",
    "            AgentResponse 객체\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"사용자 질문: {user_query}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Recovery 초기화\n",
    "        self.recovery.reset()\n",
    "        \n",
    "        # 1. 사용자 메시지를 메모리에 추가\n",
    "        self.memory.add_message(\"user\", user_query)\n",
    "        \n",
    "        # 2. 질문 복잡도 분석\n",
    "        print(\"\\n단계 1: 질문 복잡도 분석\")\n",
    "        complexity = self.complexity_analyzer.analyze(user_query)\n",
    "        print(f\"  - 복잡도: {complexity.complexity_level}\")\n",
    "        print(f\"  - 추론 깊이: {complexity.reasoning_depth}\")\n",
    "        print(f\"  - 분석 근거: {complexity.analysis_reason}\")\n",
    "        \n",
    "        # 3. 모델 선택\n",
    "        print(\"\\n단계 2: 모델 선택\")\n",
    "        selection = self.model_router.route(user_query, complexity)\n",
    "        print(f\"  - 선택된 모델: {selection.selected_model.model_tier} tier\")\n",
    "        print(f\"  - 신뢰도: {selection.confidence:.2f}\")\n",
    "        print(f\"  - 선택 이유: {selection.selection_reason}\")\n",
    "        \n",
    "        # 4. 모델 선택 검증\n",
    "        is_valid, validation_msg = self.validator.validate_model_selection(selection, complexity)\n",
    "        print(f\"\\n단계 3: 모델 선택 검증\")\n",
    "        print(f\"  - 검증 결과: {validation_msg}\")\n",
    "        \n",
    "        # 선택 검증 실패 시 대안 모델 사용\n",
    "        if not is_valid and selection.alternative_models:\n",
    "            print(\"  - 대안 모델로 전환\")\n",
    "            alternative_id = selection.alternative_models[0]\n",
    "            selection.selected_model = MODEL_REGISTRY[alternative_id]\n",
    "        \n",
    "        # 현재 사용할 모델 ID\n",
    "        current_model_id = None\n",
    "        for model_id, spec in MODEL_REGISTRY.items():\n",
    "            if spec == selection.selected_model:\n",
    "                current_model_id = model_id\n",
    "                break\n",
    "        \n",
    "        # 5. 응답 생성 (에스컬레이션 루프)\n",
    "        answer = None\n",
    "        quality = None\n",
    "        response_time_ms = 0\n",
    "        was_escalated = False\n",
    "        attempts = 0\n",
    "        max_attempts = 3\n",
    "        \n",
    "        while attempts < max_attempts:\n",
    "            attempts += 1\n",
    "            print(f\"\\n단계 4-{attempts}: 응답 생성 (모델: {current_model_id})\")\n",
    "            \n",
    "            # 응답 생성\n",
    "            answer, response_time_ms = self.generate_response(user_query, current_model_id)\n",
    "            print(f\"  - 응답 시간: {response_time_ms}ms\")\n",
    "            print(f\"  - 답변: {answer[:100]}...\" if len(answer) > 100 else f\"  - 답변: {answer}\")\n",
    "            \n",
    "            # 6. 응답 품질 검증\n",
    "            quality = self.validator.validate_response(user_query, answer, current_model_id)\n",
    "            print(f\"\\n단계 5-{attempts}: 응답 품질 검증\")\n",
    "            print(f\"  - 정확도: {quality.accuracy:.2f}\")\n",
    "            print(f\"  - 완전성: {quality.completeness:.2f}\")\n",
    "            print(f\"  - 명확성: {quality.clarity:.2f}\")\n",
    "            print(f\"  - 관련성: {quality.relevance:.2f}\")\n",
    "            print(f\"  - 종합 점수: {quality.overall_score:.2f}\")\n",
    "            \n",
    "            # 7. 에스컬레이션 판단\n",
    "            should_escalate = self.validator.should_escalate(quality)\n",
    "            \n",
    "            if not should_escalate:\n",
    "                print(\"  - 품질이 충분하다. 응답 확정.\")\n",
    "                break\n",
    "            \n",
    "            # 에스컬레이션 시도\n",
    "            print(f\"  - 품질이 낮다 ({quality.overall_score:.2f}). 에스컬레이션 시도...\")\n",
    "            next_model_id, escalation_msg = self.recovery.attempt_escalation(\n",
    "                user_query,\n",
    "                current_model_id,\n",
    "                f\"품질 점수 {quality.overall_score:.2f}\"\n",
    "            )\n",
    "            \n",
    "            if next_model_id is None:\n",
    "                print(f\"  - {escalation_msg}. 현재 응답 사용.\")\n",
    "                break\n",
    "            \n",
    "            current_model_id = next_model_id\n",
    "            was_escalated = True\n",
    "        \n",
    "        # 최종 응답이 없으면 대체 응답 생성\n",
    "        if answer is None:\n",
    "            answer = self.recovery.generate_fallback_response(user_query)\n",
    "            quality = ResponseQuality(\n",
    "                accuracy=0.0,\n",
    "                completeness=0.0,\n",
    "                clarity=0.0,\n",
    "                relevance=0.0,\n",
    "                overall_score=0.0\n",
    "            )\n",
    "        \n",
    "        # 8. 메모리에 답변 추가\n",
    "        self.memory.add_message(\n",
    "            \"assistant\",\n",
    "            answer,\n",
    "            model_used=current_model_id,\n",
    "            metadata={\n",
    "                \"quality_score\": quality.overall_score,\n",
    "                \"was_escalated\": was_escalated\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 9. 모델 사용 통계 업데이트\n",
    "        self.memory.update_model_stats(\n",
    "            current_model_id,\n",
    "            quality.overall_score,\n",
    "            quality.overall_score >= 0.7\n",
    "        )\n",
    "        \n",
    "        # 10. 피드백 기록\n",
    "        self.feedback.record_interaction(\n",
    "            user_query,\n",
    "            complexity,\n",
    "            current_model_id,\n",
    "            answer,\n",
    "            quality,\n",
    "            was_escalated\n",
    "        )\n",
    "        \n",
    "        # 11. 비용 계산\n",
    "        model_spec = MODEL_REGISTRY[current_model_id]\n",
    "        estimated_cost = model_spec.cost_per_1k_tokens * 0.5  # 평균 500 토큰 가정\n",
    "        \n",
    "        # 12. 최종 응답 생성\n",
    "        return AgentResponse(\n",
    "            answer=answer,\n",
    "            model_used=current_model_id,\n",
    "            complexity_analysis=complexity,\n",
    "            response_time_ms=response_time_ms,\n",
    "            quality_score=quality,\n",
    "            was_escalated=was_escalated,\n",
    "            total_cost_usd=estimated_cost\n",
    "        )\n",
    "\n",
    "# Router LLM Model Agent 인스턴스 생성\n",
    "agent = RouterLLMModelAgent(\n",
    "    complexity_analyzer=complexity_analyzer,\n",
    "    model_router=model_router,\n",
    "    memory=memory,\n",
    "    validator=validator,\n",
    "    recovery=recovery_system,\n",
    "    feedback=feedback_system\n",
    ")\n",
    "\n",
    "print(\"\\nRouter LLM Model Agent가 초기화되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 에이전트 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "사용자 질문: 안녕하세요\n",
      "============================================================\n",
      "\n",
      "단계 1: 질문 복잡도 분석\n",
      "  - 복잡도: simple\n",
      "  - 추론 깊이: 1\n",
      "  - 분석 근거: 단순 인사로, 별다른 정보 조회나 복잡한 요구가 없기 때문에 매우 간단하다.\n",
      "\n",
      "단계 2: 모델 선택\n",
      "  - 선택된 모델: nano tier\n",
      "  - 신뢰도: 0.95\n",
      "  - 선택 이유: 최소한의 복잡도와 빠른 응답을 제공하기 위해 경량 모델을 선택했다.\n",
      "\n",
      "단계 3: 모델 선택 검증\n",
      "  - 검증 결과: 모델 선택이 적합하다\n",
      "\n",
      "단계 4-1: 응답 생성 (모델: gpt-5-nano)\n",
      "  - 응답 시간: 6057ms\n",
      "  - 답변: \n",
      "\n",
      "단계 5-1: 응답 품질 검증\n",
      "  - 정확도: 0.00\n",
      "  - 완전성: 0.00\n",
      "  - 명확성: 0.00\n",
      "  - 관련성: 0.00\n",
      "  - 종합 점수: 0.00\n",
      "  - 품질이 낮다 (0.00). 에스컬레이션 시도...\n",
      "Recovery: gpt-5-nano → gpt-5-mini 에스컬레이션 (품질 점수 0.00)\n",
      "\n",
      "단계 4-2: 응답 생성 (모델: gpt-5-mini)\n",
      "  - 응답 시간: 11439ms\n",
      "  - 답변: 안녕하세요! 반갑습니다. 주문 도와드릴게요. 무엇을 드릴까요?\n",
      "\n",
      "추천 메뉴 몇 가지 말씀드릴게요.\n",
      "- 아메리카노: 깔끔하고 부담 없음  \n",
      "- 카페라떼: 부드러운 우유 풍미  \n",
      "- ...\n",
      "\n",
      "단계 5-2: 응답 품질 검증\n",
      "  - 정확도: 1.00\n",
      "  - 완전성: 1.00\n",
      "  - 명확성: 1.00\n",
      "  - 관련성: 1.00\n",
      "  - 종합 점수: 1.00\n",
      "  - 품질이 충분하다. 응답 확정.\n",
      "\n",
      "============================================================\n",
      "최종 응답 요약\n",
      "============================================================\n",
      "답변: 안녕하세요! 반갑습니다. 주문 도와드릴게요. 무엇을 드릴까요?\n",
      "\n",
      "추천 메뉴 몇 가지 말씀드릴게요.\n",
      "- 아메리카노: 깔끔하고 부담 없음  \n",
      "- 카페라떼: 부드러운 우유 풍미  \n",
      "- 카라멜 마끼아또: 달콤한 토핑 원하실 때  \n",
      "- 시즌 음료: 계절 한정 맛(원하시면 지금 메뉴 알려드릴게요)\n",
      "\n",
      "옵션: 사이즈(스몰/미디움/라지), HOT/ICED, 샷 추가, 시럽/당도 조절, 우유 변경(오트/저지방/두유) 등 가능해요. 결제는 카드/모바일/현금 모두 되고, 매장 드실지 포장할지도 알려주세요. 알레르기나 기호가 있으시면 미리 알려주세요.\n",
      "\n",
      "원하시는 메뉴 알려주시면 바로 주문 도와드릴게요.\n",
      "사용 모델: gpt-5-mini\n",
      "복잡도: simple\n",
      "응답 시간: 11439ms\n",
      "품질 점수: 1.00\n",
      "에스컬레이션: 예\n",
      "예상 비용: $0.000150\n",
      "\n",
      "\n",
      "============================================================\n",
      "사용자 질문: 아메리카노 가격은 얼마인가요?\n",
      "============================================================\n",
      "\n",
      "단계 1: 질문 복잡도 분석\n",
      "  - 복잡도: simple\n",
      "  - 추론 깊이: 1\n",
      "  - 분석 근거: 아메리카노 가격에 대한 단순한 정보 조회이므로 복잡도가 낮고 즉각적인 응답이 필요하다.\n",
      "\n",
      "단계 2: 모델 선택\n",
      "  - 선택된 모델: nano tier\n",
      "  - 신뢰도: 0.95\n",
      "  - 선택 이유: 아메리카노 가격에 대한 단순한 정보 조회로 복잡도가 낮고 즉각적인 응답이 필요하기 때문에 경량 모델인 gpt-5-nano를 선택하는 것이 적합하다.\n",
      "\n",
      "단계 3: 모델 선택 검증\n",
      "  - 검증 결과: 모델 선택이 적합하다\n",
      "\n",
      "단계 4-1: 응답 생성 (모델: gpt-5-nano)\n",
      "  - 응답 시간: 6520ms\n",
      "  - 답변: \n",
      "\n",
      "단계 5-1: 응답 품질 검증\n",
      "  - 정확도: 0.00\n",
      "  - 완전성: 0.00\n",
      "  - 명확성: 0.00\n",
      "  - 관련성: 0.00\n",
      "  - 종합 점수: 0.00\n",
      "  - 품질이 낮다 (0.00). 에스컬레이션 시도...\n",
      "Recovery: gpt-5-nano → gpt-5-mini 에스컬레이션 (품질 점수 0.00)\n",
      "\n",
      "단계 4-2: 응답 생성 (모델: gpt-5-mini)\n",
      "  - 응답 시간: 8249ms\n",
      "  - 답변: 매장과 사이즈에 따라 달라요. 어떤 사이즈로 드릴까요? (스몰/미디움/라지)  \n",
      "참고로 일반적인 가격대는 보통 스몰 2,500원, 미디움 3,000원, 라지 3,500원 정도이고,...\n",
      "\n",
      "단계 5-2: 응답 품질 검증\n",
      "  - 정확도: 1.00\n",
      "  - 완전성: 1.00\n",
      "  - 명확성: 1.00\n",
      "  - 관련성: 1.00\n",
      "  - 종합 점수: 1.00\n",
      "  - 품질이 충분하다. 응답 확정.\n",
      "\n",
      "============================================================\n",
      "최종 응답 요약\n",
      "============================================================\n",
      "답변: 매장과 사이즈에 따라 달라요. 어떤 사이즈로 드릴까요? (스몰/미디움/라지)  \n",
      "참고로 일반적인 가격대는 보통 스몰 2,500원, 미디움 3,000원, 라지 3,500원 정도이고, 아이스/샷 추가/시럽 등은 추가요금이 붙습니다.  \n",
      "원하시는 사이즈나 지금 계신 지점 알려주시면 정확한 가격 확인해드릴게요.\n",
      "사용 모델: gpt-5-mini\n",
      "복잡도: simple\n",
      "응답 시간: 8249ms\n",
      "품질 점수: 1.00\n",
      "에스컬레이션: 예\n",
      "예상 비용: $0.000150\n",
      "\n",
      "\n",
      "============================================================\n",
      "사용자 질문: 카페라떼와 카푸치노 중 뭐가 더 부드러운가요?\n",
      "============================================================\n",
      "\n",
      "단계 1: 질문 복잡도 분석\n",
      "  - 복잡도: moderate\n",
      "  - 추론 깊이: 3\n",
      "  - 분석 근거: 카페라떼와 카푸치노의 비교를 요구하는 질문으로, 두 메뉴 간의 특징을 설명하고 추천하는 데에 어느 정도의 복잡성이 있다.\n",
      "\n",
      "단계 2: 모델 선택\n",
      "  - 선택된 모델: mini tier\n",
      "  - 신뢰도: 0.95\n",
      "  - 선택 이유: 질문은 카페라떼와 카푸치노의 비교를 요구하므로, 표준 모델(gpt-5-mini)이 적합하여 적절한 정보 제공이 가능하다. 이 모델은 균형 잡힌 응답 시간과 품질을 제공하며, 과거 성능에서도 100% 성공률을 기록하였다.\n",
      "\n",
      "단계 3: 모델 선택 검증\n",
      "  - 검증 결과: 모델 선택이 적합하다\n",
      "\n",
      "단계 4-1: 응답 생성 (모델: gpt-5-mini)\n",
      "  - 응답 시간: 6602ms\n",
      "  - 답변: 짧게 말하면: 카페라떼가 더 부드러워요.\n",
      "\n",
      "간단한 차이점 설명드릴게요.\n",
      "- 카페라떼: 에스프레소에 많은 스팀밀크(부드러운 우유)와 얇은 거품이 들어가서 우유 맛이 강하고 크리미해서...\n",
      "\n",
      "단계 5-1: 응답 품질 검증\n",
      "  - 정확도: 1.00\n",
      "  - 완전성: 1.00\n",
      "  - 명확성: 1.00\n",
      "  - 관련성: 1.00\n",
      "  - 종합 점수: 1.00\n",
      "  - 품질이 충분하다. 응답 확정.\n",
      "\n",
      "============================================================\n",
      "최종 응답 요약\n",
      "============================================================\n",
      "답변: 짧게 말하면: 카페라떼가 더 부드러워요.\n",
      "\n",
      "간단한 차이점 설명드릴게요.\n",
      "- 카페라떼: 에스프레소에 많은 스팀밀크(부드러운 우유)와 얇은 거품이 들어가서 우유 맛이 강하고 크리미해서 입안에서 부드럽게 느껴져요.  \n",
      "- 카푸치노: 에스프레소·스팀밀크·풍성한 거품이 1:1:1 비슷하게 섞여 거품감이 있고 커피 맛이 상대적으로 더 진하게 느껴집니다(텍스처는 더 ‘가벼움’).  \n",
      "\n",
      "추가 팁\n",
      "- 더 부드럽게 원하시면 전지우유나 오트밀크 선택, 또는 라떼/플랫화이트(더 미세한 마이크로폼) 추천드려요.  \n",
      "- 강한 커피 맛을 원하면 카푸치노가 좋아요.\n",
      "\n",
      "어떤 쪽으로 드릴까요? 주문 도와드릴게요.\n",
      "사용 모델: gpt-5-mini\n",
      "복잡도: moderate\n",
      "응답 시간: 6602ms\n",
      "품질 점수: 1.00\n",
      "에스컬레이션: 아니오\n",
      "예상 비용: $0.000150\n",
      "\n",
      "\n",
      "============================================================\n",
      "사용자 질문: 피곤한데 잠이 안 오면서도 집중력을 높일 수 있는 음료를 추천해주세요. 우유는 먹을 수 있지만 너무 달지 않았으면 좋겠어요.\n",
      "============================================================\n",
      "\n",
      "단계 1: 질문 복잡도 분석\n",
      "  - 복잡도: complex\n",
      "  - 추론 깊이: 5\n",
      "  - 분석 근거: 고객이 제시한 여러 조건(피곤함, 집중력 증가, 우유 가능, 낮은 단맛)을 만족하는 음료를 맞춤 추천해야 하므로 복잡도가 높고 창의적인 해결이 필요하다.\n",
      "\n",
      "단계 2: 모델 선택\n",
      "  - 선택된 모델: standard tier\n",
      "  - 신뢰도: 1.00\n",
      "  - 선택 이유: 고객의 여러 조건을 충족하는 맞춤형 음료 추천이 필요하고, 복잡한 추론과 창의력이 요구되기 때문.\n",
      "\n",
      "단계 3: 모델 선택 검증\n",
      "  - 검증 결과: 모델 선택이 적합하다\n",
      "\n",
      "단계 4-1: 응답 생성 (모델: gpt-4o-mini)\n",
      "  - 응답 시간: 5517ms\n",
      "  - 답변: 피곤할 때 집중력을 높이면서 우유도 포함된 음료로는 '아메리카노에 스팀밀크를 추가한 라떼' 또는 '플랫화이트'를 추천드려요.\n",
      "\n",
      "1. **아메리카노 + 스팀밀크**: 아메리카노의 진...\n",
      "\n",
      "단계 5-1: 응답 품질 검증\n",
      "  - 정확도: 1.00\n",
      "  - 완전성: 1.00\n",
      "  - 명확성: 1.00\n",
      "  - 관련성: 1.00\n",
      "  - 종합 점수: 1.00\n",
      "  - 품질이 충분하다. 응답 확정.\n",
      "\n",
      "============================================================\n",
      "최종 응답 요약\n",
      "============================================================\n",
      "답변: 피곤할 때 집중력을 높이면서 우유도 포함된 음료로는 '아메리카노에 스팀밀크를 추가한 라떼' 또는 '플랫화이트'를 추천드려요.\n",
      "\n",
      "1. **아메리카노 + 스팀밀크**: 아메리카노의 진한 커피 맛과 스팀밀크의 부드러움이 조화를 이루면서 약간의 부드러운 맛이 더해집니다. 단맛이 강하지 않고 커피 본연의 맛을 느낄 수 있어요.\n",
      "\n",
      "2. **플랫화이트**: 에스프레소에 스팀밀크가 적절히 섞여 부드럽고 진한 커피 맛을 제공합니다. 또, 거품이 아주 부드럽기 때문에 우유를 좋아하신다면 만족스러우실 거예요.\n",
      "\n",
      "이 두 음료 모두 당도를 조절할 수 있으니 원하시면 시럽을 추가하지 않고 주문할 수 있습니다. 어떤 음료가 더 마음에 드시나요? 주문 도와드릴게요!\n",
      "사용 모델: gpt-4o-mini\n",
      "복잡도: complex\n",
      "응답 시간: 5517ms\n",
      "품질 점수: 1.00\n",
      "에스컬레이션: 아니오\n",
      "예상 비용: $0.002500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 질문 리스트 (복잡도 다양화)\n",
    "test_queries = [\n",
    "    \"안녕하세요\",  # Simple\n",
    "    \"아메리카노 가격은 얼마인가요?\",  # Simple\n",
    "    \"카페라떼와 카푸치노 중 뭐가 더 부드러운가요?\",  # Moderate\n",
    "    \"피곤한데 잠이 안 오면서도 집중력을 높일 수 있는 음료를 추천해주세요. 우유는 먹을 수 있지만 너무 달지 않았으면 좋겠어요.\",  # Complex\n",
    "]\n",
    "\n",
    "# 각 질문에 대해 에이전트 실행\n",
    "for query in test_queries:\n",
    "    response = agent.process_query(query)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"최종 응답 요약\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"답변: {response.answer}\")\n",
    "    print(f\"사용 모델: {response.model_used}\")\n",
    "    print(f\"복잡도: {response.complexity_analysis.complexity_level}\")\n",
    "    print(f\"응답 시간: {response.response_time_ms}ms\")\n",
    "    print(f\"품질 점수: {response.quality_score.overall_score:.2f}\")\n",
    "    print(f\"에스컬레이션: {'예' if response.was_escalated else '아니오'}\")\n",
    "    print(f\"예상 비용: ${response.total_cost_usd:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 대화형 인터페이스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "커피 키오스크 Router LLM Model Agent\n",
      "종료하려면 'quit' 또는 'exit'를 입력하세요.\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "고객:  뜨거운 아메리카노를 에스프레소 2샷 넣어서 만들어주세요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "사용자 질문: 뜨거운 아메리카노를 에스프레소 2샷 넣어서 만들어주세요\n",
      "============================================================\n",
      "\n",
      "단계 1: 질문 복잡도 분석\n",
      "  - 복잡도: simple\n",
      "  - 추론 깊이: 1\n",
      "  - 분석 근거: 고객이 요청하는 음료의 종류와 조합이 명확하여 단순한 주문으로 분류된다.\n",
      "\n",
      "단계 2: 모델 선택\n",
      "  - 선택된 모델: nano tier\n",
      "  - 신뢰도: 0.95\n",
      "  - 선택 이유: 주문 요청이 간단하고 명확하여 빠르고 저비용의 경량 모델이 최적이다.\n",
      "\n",
      "단계 3: 모델 선택 검증\n",
      "  - 검증 결과: 모델 선택이 적합하다\n",
      "\n",
      "단계 4-1: 응답 생성 (모델: gpt-5-nano)\n",
      "  - 응답 시간: 6048ms\n",
      "  - 답변: \n",
      "\n",
      "단계 5-1: 응답 품질 검증\n",
      "  - 정확도: 1.00\n",
      "  - 완전성: 1.00\n",
      "  - 명확성: 1.00\n",
      "  - 관련성: 1.00\n",
      "  - 종합 점수: 1.00\n",
      "  - 품질이 충분하다. 응답 확정.\n",
      "\n",
      "에이전트: \n",
      "[gpt-5-nano | 품질: 1.00 | 6048ms]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "고객:  카페라떼에 우유 대신에 두유를 넣으면서 라지 사이즈 3잔으로 주문할게요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "사용자 질문: 카페라떼에 우유 대신에 두유를 넣으면서 라지 사이즈 3잔으로 주문할게요\n",
      "============================================================\n",
      "\n",
      "단계 1: 질문 복잡도 분석\n",
      "  - 복잡도: complex\n",
      "  - 추론 깊이: 4\n",
      "  - 분석 근거: 고객의 주문은 특정 옵션(우유 대신 두유)과 수량(3잔)을 포함한 복합적인 요청이므로 복잡도가 높다.\n",
      "\n",
      "단계 2: 모델 선택\n",
      "  - 선택된 모델: standard tier\n",
      "  - 신뢰도: 0.95\n",
      "  - 선택 이유: 주문의 복잡성이 높고, 특정 옵션 및 수량을 포함한 맞춤형 주문으로, 고성능 모델이 필요하다.\n",
      "\n",
      "단계 3: 모델 선택 검증\n",
      "  - 검증 결과: 모델 선택이 적합하다\n",
      "\n",
      "단계 4-1: 응답 생성 (모델: gpt-4o-mini)\n",
      "  - 응답 시간: 2396ms\n",
      "  - 답변: 주문 내용 확인했습니다! \n",
      "\n",
      "- **카페라떼**, 라지 사이즈 3잔 \n",
      "- **두유로 대체**\n",
      "\n",
      "주문이 잘 준비될 거예요. 혹시 추가로 시럽이나 다른 옵션이 필요하신가요? 아니면 결...\n",
      "\n",
      "단계 5-1: 응답 품질 검증\n",
      "  - 정확도: 1.00\n",
      "  - 완전성: 0.90\n",
      "  - 명확성: 1.00\n",
      "  - 관련성: 1.00\n",
      "  - 종합 점수: 0.97\n",
      "  - 품질이 충분하다. 응답 확정.\n",
      "\n",
      "에이전트: 주문 내용 확인했습니다! \n",
      "\n",
      "- **카페라떼**, 라지 사이즈 3잔 \n",
      "- **두유로 대체**\n",
      "\n",
      "주문이 잘 준비될 거예요. 혹시 추가로 시럽이나 다른 옵션이 필요하신가요? 아니면 결제 방법도 알려주시면 좋을 것 같아요!\n",
      "[gpt-4o-mini | 품질: 0.97 | 2396ms]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "고객:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "대화를 종료한다.\n"
     ]
    }
   ],
   "source": [
    "def chat_interface():\n",
    "    \"\"\"대화형 인터페이스 함수\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"커피 키오스크 Router LLM Model Agent\")\n",
    "    print(\"종료하려면 'quit' 또는 'exit'를 입력하세요.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\n고객: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', '종료']:\n",
    "            print(\"\\n대화를 종료한다.\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        response = agent.process_query(user_input)\n",
    "        print(f\"\\n에이전트: {response.answer}\")\n",
    "        print(f\"[{response.model_used} | 품질: {response.quality_score.overall_score:.2f} | {response.response_time_ms}ms]\")\n",
    "\n",
    "# 대화 시작 (주석을 제거하여 실행)\n",
    "chat_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 성능 분석 및 리포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "성능 분석 리포트\n",
      "============================================================\n",
      "\n",
      "1. 모델별 성능\n",
      "\n",
      "  gpt-5-nano:\n",
      "    - 사용 횟수: 1회\n",
      "    - 평균 품질: 1.00\n",
      "    - 에스컬레이션 비율: 0.0%\n",
      "\n",
      "  gpt-5-mini:\n",
      "    - 사용 횟수: 3회\n",
      "    - 평균 품질: 1.00\n",
      "    - 에스컬레이션 비율: 66.7%\n",
      "\n",
      "  gpt-4o-mini:\n",
      "    - 사용 횟수: 2회\n",
      "    - 평균 품질: 0.99\n",
      "    - 에스컬레이션 비율: 0.0%\n",
      "\n",
      "2. 비용 분석\n",
      "  - 총 비용: $0.005525\n",
      "  - 쿼리당 평균 비용: $0.000921\n",
      "  - 처리한 쿼리 수: 6개\n",
      "\n",
      "3. 최적화 제안\n",
      "  1. gpt-5-mini의 에스컬레이션 비율이 66.7%로 높다. 이 모델의 사용 기준을 재검토하라.\n",
      "  2. 전체 에스컬레이션 비율이 33.3%다. 초기 모델 선택 기준을 상향 조정하는 것을 고려하라.\n",
      "\n",
      "4. 에스컬레이션 통계\n",
      "  - 총 에스컬레이션 횟수: 0회\n",
      "\n",
      "5. 메모리 상태\n",
      "  - 메시지: 10개, 모델 호출: 6회\n",
      "\n",
      "============================================================\n",
      "튜토리얼 2: Router LLM Model Agent 완료\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"성능 분석 리포트\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 모델별 성능 분석\n",
    "print(\"\\n1. 모델별 성능\")\n",
    "performance = feedback_system.analyze_model_performance()\n",
    "for model_id, stats in performance.items():\n",
    "    print(f\"\\n  {model_id}:\")\n",
    "    print(f\"    - 사용 횟수: {stats['usage_count']}회\")\n",
    "    print(f\"    - 평균 품질: {stats['avg_quality']:.2f}\")\n",
    "    print(f\"    - 에스컬레이션 비율: {stats['escalation_rate']:.1%}\")\n",
    "\n",
    "# 비용 분석\n",
    "print(\"\\n2. 비용 분석\")\n",
    "cost_analysis = feedback_system.get_cost_analysis()\n",
    "print(f\"  - 총 비용: ${cost_analysis['total_cost']:.6f}\")\n",
    "print(f\"  - 쿼리당 평균 비용: ${cost_analysis['avg_cost_per_query']:.6f}\")\n",
    "print(f\"  - 처리한 쿼리 수: {cost_analysis['query_count']}개\")\n",
    "\n",
    "# 최적화 제안\n",
    "print(\"\\n3. 최적화 제안\")\n",
    "suggestions = feedback_system.get_optimization_suggestions()\n",
    "for i, suggestion in enumerate(suggestions, 1):\n",
    "    print(f\"  {i}. {suggestion}\")\n",
    "\n",
    "# 에스컬레이션 통계\n",
    "print(\"\\n4. 에스컬레이션 통계\")\n",
    "escalation_stats = recovery_system.get_escalation_stats()\n",
    "print(f\"  - 총 에스컬레이션 횟수: {escalation_stats['count']}회\")\n",
    "if escalation_stats['most_common_reason']:\n",
    "    print(f\"  - 가장 흔한 이유: {escalation_stats['most_common_reason']}\")\n",
    "\n",
    "# 메모리 상태\n",
    "print(\"\\n5. 메모리 상태\")\n",
    "print(f\"  - {memory.get_summary()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"튜토리얼 2: Router LLM Model Agent 완료\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
