{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Router LLM Serving Agent - 커피 키오스크 주문 시스템"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env 파일에서 환경 변수를 로드한다\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "# !pip install openai pydantic requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from typing import List, Dict, Optional, Literal, Any\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from enum import Enum\n",
    "\n",
    "# OpenAI 클라이언트 초기화\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pydantic 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServingType(str, Enum):\n",
    "    \"\"\"LLM 서빙 타입 열거형\"\"\"\n",
    "    CLOUD = \"cloud\"\n",
    "    LOCAL = \"local\"\n",
    "\n",
    "\n",
    "class RequestCharacteristics(BaseModel):\n",
    "    \"\"\"요청의 특성을 분석한 결과를 표현하는 모델\"\"\"\n",
    "    complexity: Literal[\"simple\", \"moderate\", \"complex\"] = Field(\n",
    "        description=\"질문의 복잡도\"\n",
    "    )\n",
    "    privacy_sensitivity: Literal[\"low\", \"medium\", \"high\"] = Field(\n",
    "        description=\"프라이버시 민감도\"\n",
    "    )\n",
    "    response_time_requirement: Literal[\"fast\", \"balanced\", \"quality\"] = Field(\n",
    "        description=\"응답 시간 요구사항\"\n",
    "    )\n",
    "    contains_sensitive_data: bool = Field(\n",
    "        description=\"민감 정보 포함 여부\"\n",
    "    )\n",
    "    estimated_tokens: int = Field(\n",
    "        ge=0,\n",
    "        description=\"예상 토큰 수\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"분석 근거\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ServingEndpoint(BaseModel):\n",
    "    \"\"\"LLM 서빙 엔드포인트 정보를 표현하는 모델\"\"\"\n",
    "    serving_type: ServingType = Field(description=\"서빙 타입\")\n",
    "    endpoint_name: str = Field(description=\"엔드포인트 이름\")\n",
    "    model_name: str = Field(description=\"모델 이름\")\n",
    "    is_available: bool = Field(default=True, description=\"서비스 가용 여부\")\n",
    "    avg_latency_ms: int = Field(ge=0, description=\"평균 지연시간 (밀리초)\")\n",
    "    cost_per_1k_tokens: float = Field(ge=0, description=\"1K 토큰당 비용 (USD)\")\n",
    "    max_context_length: int = Field(description=\"최대 컨텍스트 길이\")\n",
    "    supports_streaming: bool = Field(description=\"스트리밍 지원 여부\")\n",
    "    privacy_level: Literal[\"cloud\", \"local\"] = Field(description=\"프라이버시 레벨\")\n",
    "\n",
    "\n",
    "class ServingSelection(BaseModel):\n",
    "    \"\"\"서빙 선택 결과를 표현하는 모델\"\"\"\n",
    "    selected_endpoint: ServingEndpoint = Field(description=\"선택된 엔드포인트\")\n",
    "    selection_reason: str = Field(description=\"선택 이유\")\n",
    "    confidence: float = Field(ge=0.0, le=1.0, description=\"선택 신뢰도\")\n",
    "    fallback_endpoint: Optional[ServingEndpoint] = Field(\n",
    "        default=None,\n",
    "        description=\"대체 엔드포인트\"\n",
    "    )\n",
    "    estimated_cost: float = Field(ge=0, description=\"예상 비용 (USD)\")\n",
    "\n",
    "\n",
    "class ServingResponse(BaseModel):\n",
    "    \"\"\"LLM 서빙 응답을 표현하는 모델\"\"\"\n",
    "    content: str = Field(description=\"응답 내용\")\n",
    "    serving_type: ServingType = Field(description=\"사용된 서빙 타입\")\n",
    "    model_name: str = Field(description=\"사용된 모델 이름\")\n",
    "    latency_ms: int = Field(description=\"실제 지연시간 (밀리초)\")\n",
    "    tokens_used: int = Field(description=\"사용된 토큰 수\")\n",
    "    actual_cost: float = Field(description=\"실제 비용 (USD)\")\n",
    "    was_fallback: bool = Field(description=\"폴백 사용 여부\")\n",
    "    timestamp: datetime = Field(default_factory=datetime.now, description=\"응답 시간\")\n",
    "\n",
    "\n",
    "class ServiceHealth(BaseModel):\n",
    "    \"\"\"서비스 헬스 체크 결과를 표현하는 모델\"\"\"\n",
    "    serving_type: ServingType = Field(description=\"서빙 타입\")\n",
    "    is_healthy: bool = Field(description=\"서비스 정상 여부\")\n",
    "    response_time_ms: Optional[int] = Field(default=None, description=\"응답 시간\")\n",
    "    error_message: Optional[str] = Field(default=None, description=\"오류 메시지\")\n",
    "    last_check: datetime = Field(default_factory=datetime.now, description=\"마지막 체크 시간\")\n",
    "\n",
    "\n",
    "class ConversationMessage(BaseModel):\n",
    "    \"\"\"대화 메시지를 표현하는 모델\"\"\"\n",
    "    role: Literal[\"user\", \"assistant\", \"system\"] = Field(description=\"메시지 역할\")\n",
    "    content: str = Field(description=\"메시지 내용\")\n",
    "    timestamp: datetime = Field(default_factory=datetime.now, description=\"메시지 생성 시간\")\n",
    "    serving_metadata: Optional[Dict] = Field(default=None, description=\"서빙 메타데이터\")\n",
    "\n",
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    \"\"\"에이전트의 최종 응답을 표현하는 모델\"\"\"\n",
    "    answer: str = Field(description=\"사용자에게 제공되는 답변\")\n",
    "    serving_info: ServingResponse = Field(description=\"서빙 정보\")\n",
    "    request_analysis: RequestCharacteristics = Field(description=\"요청 분석 결과\")\n",
    "    cost_savings: float = Field(description=\"로컬 사용으로 절감된 비용 (USD)\")\n",
    "    recommendation: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"사용자를 위한 추천 사항\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM 서빙 엔드포인트 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 2개의 서빙 엔드포인트가 등록되었다.\n",
      "  - OpenAI GPT-4o-mini (cloud): 지연시간 800ms, 비용 $0.00030/1K tokens\n",
      "  - Ollama exaone3.5 (local): 지연시간 300ms, 비용 $0.00000/1K tokens\n"
     ]
    }
   ],
   "source": [
    "# 서빙 엔드포인트 레지스트리\n",
    "SERVING_ENDPOINTS = {\n",
    "    \"cloud_gpt4o_mini\": ServingEndpoint(\n",
    "        serving_type=ServingType.CLOUD,\n",
    "        endpoint_name=\"OpenAI GPT-4o-mini\",\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        is_available=True,\n",
    "        avg_latency_ms=800,\n",
    "        cost_per_1k_tokens=0.00030,\n",
    "        max_context_length=128000,\n",
    "        supports_streaming=True,\n",
    "        privacy_level=\"cloud\"\n",
    "    ),\n",
    "    \"local_ollama_llama3\": ServingEndpoint(\n",
    "        serving_type=ServingType.LOCAL,\n",
    "        endpoint_name=\"Ollama exaone3.5\",\n",
    "        model_name=\"exaone3.5:7.8\",\n",
    "        is_available=True,  # 실제로는 Ollama 설치 여부에 따라 결정\n",
    "        avg_latency_ms=300,\n",
    "        cost_per_1k_tokens=0.0,  # 로컬은 무료\n",
    "        max_context_length=8192,\n",
    "        supports_streaming=True,\n",
    "        privacy_level=\"local\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Ollama 로컬 서버 URL (기본값)\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "print(f\"총 {len(SERVING_ENDPOINTS)}개의 서빙 엔드포인트가 등록되었다.\")\n",
    "for endpoint_id, endpoint in SERVING_ENDPOINTS.items():\n",
    "    print(f\"  - {endpoint.endpoint_name} ({endpoint.serving_type.value}): \"\n",
    "          f\"지연시간 {endpoint.avg_latency_ms}ms, \"\n",
    "          f\"비용 ${endpoint.cost_per_1k_tokens:.5f}/1K tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory 시스템 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서빙 메모리가 초기화되었다.\n"
     ]
    }
   ],
   "source": [
    "class ServingMemory:\n",
    "    \"\"\"서빙 이력과 성능 통계를 관리하는 메모리 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, max_history: int = 20):\n",
    "        self.messages: List[ConversationMessage] = []\n",
    "        self.max_history = max_history\n",
    "        self.serving_history: List[ServingResponse] = []\n",
    "        self.service_stats: Dict[str, Dict] = {}\n",
    "        \n",
    "        # 각 서비스의 통계 초기화\n",
    "        for endpoint_id in SERVING_ENDPOINTS.keys():\n",
    "            self.service_stats[endpoint_id] = {\n",
    "                \"total_requests\": 0,\n",
    "                \"successful_requests\": 0,\n",
    "                \"failed_requests\": 0,\n",
    "                \"total_latency_ms\": 0,\n",
    "                \"total_tokens\": 0,\n",
    "                \"total_cost\": 0.0,\n",
    "                \"avg_latency_ms\": 0,\n",
    "                \"success_rate\": 0.0\n",
    "            }\n",
    "    \n",
    "    def add_message(self, role: str, content: str, serving_metadata: Dict = None):\n",
    "        \"\"\"새로운 메시지를 메모리에 추가한다\"\"\"\n",
    "        message = ConversationMessage(\n",
    "            role=role,\n",
    "            content=content,\n",
    "            serving_metadata=serving_metadata\n",
    "        )\n",
    "        self.messages.append(message)\n",
    "        \n",
    "        # 최대 이력 수를 초과하면 오래된 메시지부터 제거한다\n",
    "        if len(self.messages) > self.max_history:\n",
    "            self.messages = self.messages[-self.max_history:]\n",
    "    \n",
    "    def record_serving_response(self, response: ServingResponse, success: bool):\n",
    "        \"\"\"\n",
    "        서빙 응답을 기록하고 통계를 업데이트한다\n",
    "        \n",
    "        Args:\n",
    "            response: 서빙 응답\n",
    "            success: 성공 여부\n",
    "        \"\"\"\n",
    "        self.serving_history.append(response)\n",
    "        \n",
    "        # 엔드포인트 ID 찾기\n",
    "        endpoint_id = None\n",
    "        for eid, endpoint in SERVING_ENDPOINTS.items():\n",
    "            if (endpoint.serving_type == response.serving_type and \n",
    "                endpoint.model_name == response.model_name):\n",
    "                endpoint_id = eid\n",
    "                break\n",
    "        \n",
    "        if endpoint_id and endpoint_id in self.service_stats:\n",
    "            stats = self.service_stats[endpoint_id]\n",
    "            stats[\"total_requests\"] += 1\n",
    "            \n",
    "            if success:\n",
    "                stats[\"successful_requests\"] += 1\n",
    "            else:\n",
    "                stats[\"failed_requests\"] += 1\n",
    "            \n",
    "            stats[\"total_latency_ms\"] += response.latency_ms\n",
    "            stats[\"total_tokens\"] += response.tokens_used\n",
    "            stats[\"total_cost\"] += response.actual_cost\n",
    "            \n",
    "            # 평균 계산\n",
    "            if stats[\"total_requests\"] > 0:\n",
    "                stats[\"avg_latency_ms\"] = stats[\"total_latency_ms\"] / stats[\"total_requests\"]\n",
    "                stats[\"success_rate\"] = stats[\"successful_requests\"] / stats[\"total_requests\"]\n",
    "    \n",
    "    def get_service_performance(self, endpoint_id: str) -> Dict:\n",
    "        \"\"\"특정 서비스의 성능 통계를 반환한다\"\"\"\n",
    "        return self.service_stats.get(endpoint_id, {})\n",
    "    \n",
    "    def get_total_cost(self) -> float:\n",
    "        \"\"\"전체 서비스 사용 비용을 계산한다\"\"\"\n",
    "        return sum(stats[\"total_cost\"] for stats in self.service_stats.values())\n",
    "    \n",
    "    def get_cost_savings(self) -> float:\n",
    "        \"\"\"\n",
    "        로컬 서비스 사용으로 절감된 비용을 계산한다\n",
    "        \n",
    "        Returns:\n",
    "            절감된 비용 (USD)\n",
    "        \"\"\"\n",
    "        local_requests = 0\n",
    "        local_tokens = 0\n",
    "        \n",
    "        # 로컬 서비스 사용량 집계\n",
    "        for endpoint_id, stats in self.service_stats.items():\n",
    "            endpoint = SERVING_ENDPOINTS[endpoint_id]\n",
    "            if endpoint.serving_type == ServingType.LOCAL:\n",
    "                local_requests += stats[\"total_requests\"]\n",
    "                local_tokens += stats[\"total_tokens\"]\n",
    "        \n",
    "        # 클라우드 비용으로 환산\n",
    "        cloud_endpoint = SERVING_ENDPOINTS[\"cloud_gpt4o_mini\"]\n",
    "        would_be_cost = (local_tokens / 1000) * cloud_endpoint.cost_per_1k_tokens\n",
    "        \n",
    "        return would_be_cost\n",
    "    \n",
    "    def get_context(self) -> List[Dict]:\n",
    "        \"\"\"OpenAI API 형식으로 대화 이력을 반환한다\"\"\"\n",
    "        context = []\n",
    "        for msg in self.messages:\n",
    "            context.append({\n",
    "                \"role\": msg.role,\n",
    "                \"content\": msg.content\n",
    "            })\n",
    "        return context\n",
    "    \n",
    "    def get_summary(self) -> str:\n",
    "        \"\"\"메모리 상태 요약을 반환한다\"\"\"\n",
    "        total_requests = sum(stats[\"total_requests\"] for stats in self.service_stats.values())\n",
    "        total_cost = self.get_total_cost()\n",
    "        cost_savings = self.get_cost_savings()\n",
    "        return f\"총 요청: {total_requests}회, 비용: ${total_cost:.4f}, 절감: ${cost_savings:.4f}\"\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"모든 이력을 삭제한다\"\"\"\n",
    "        self.messages = []\n",
    "        self.serving_history = []\n",
    "\n",
    "# 메모리 인스턴스 생성\n",
    "memory = ServingMemory(max_history=20)\n",
    "\n",
    "# 시스템 메시지 추가\n",
    "memory.add_message(\n",
    "    \"system\",\n",
    "    \"당신은 커피 키오스크의 주문 도우미다. 고객의 질문에 친절하고 정확하게 답변하라.\"\n",
    ")\n",
    "\n",
    "print(\"서빙 메모리가 초기화되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 요청 특성 분석 시스템 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요청 분석 시스템이 초기화되었다.\n"
     ]
    }
   ],
   "source": [
    "class RequestAnalyzer:\n",
    "    \"\"\"요청 특성 분석 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.analyzer_model = \"gpt-4o-mini\"\n",
    "    \n",
    "    def analyze_request(self, query: str, context: List[Dict] = None) -> RequestCharacteristics:\n",
    "        \"\"\"\n",
    "        사용자 요청의 특성을 분석한다\n",
    "        \n",
    "        Args:\n",
    "            query: 사용자 질문\n",
    "            context: 대화 컨텍스트\n",
    "        \n",
    "        Returns:\n",
    "            RequestCharacteristics 객체\n",
    "        \"\"\"\n",
    "        analysis_prompt = f\"\"\"\n",
    "다음 커피 키오스크 고객 질문의 특성을 분석하라.\n",
    "\n",
    "질문: \"{query}\"\n",
    "\n",
    "다음 기준으로 평가하라:\n",
    "\n",
    "1. complexity (복잡도):\n",
    "   - simple: 단순 질문 (가격, 메뉴 확인, 인사)\n",
    "   - moderate: 중간 복잡도 (메뉴 비교, 추천)\n",
    "   - complex: 복잡한 질문 (맞춤형 조합, 다단계 추론)\n",
    "\n",
    "2. privacy_sensitivity (프라이버시 민감도):\n",
    "   - low: 민감 정보 없음\n",
    "   - medium: 선호도 정보\n",
    "   - high: 개인정보, 결제정보, 건강정보\n",
    "\n",
    "3. response_time_requirement (응답 시간 요구사항):\n",
    "   - fast: 즉각 응답 필요 (단순 확인)\n",
    "   - balanced: 균형 (일반 질문)\n",
    "   - quality: 품질 우선 (복잡한 추천)\n",
    "\n",
    "4. contains_sensitive_data: 민감 정보 포함 여부 (true/false)\n",
    "\n",
    "5. estimated_tokens: 예상 토큰 수 (응답 길이 예측)\n",
    "\n",
    "6. reasoning: 분석 근거 (한 문장)\n",
    "\n",
    "JSON 형식으로만 답변하라:\n",
    "{{\n",
    "  \"complexity\": \"simple|moderate|complex\",\n",
    "  \"privacy_sensitivity\": \"low|medium|high\",\n",
    "  \"response_time_requirement\": \"fast|balanced|quality\",\n",
    "  \"contains_sensitive_data\": true|false,\n",
    "  \"estimated_tokens\": 숫자,\n",
    "  \"reasoning\": \"분석 근거\"\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        messages = context or []\n",
    "        messages.append({\"role\": \"user\", \"content\": analysis_prompt})\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=self.analyzer_model,\n",
    "            messages=messages,\n",
    "            temperature=0.1,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        \n",
    "        return RequestCharacteristics(**result)\n",
    "\n",
    "# 요청 분석기 인스턴스 생성\n",
    "request_analyzer = RequestAnalyzer()\n",
    "print(\"요청 분석 시스템이 초기화되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Function Calling & Tool 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 2개의 서빙 선택 도구가 정의되었다.\n"
     ]
    }
   ],
   "source": [
    "# 서빙 선택을 위한 도구 정의\n",
    "SERVING_SELECTION_TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"select_cloud_serving\",\n",
    "            \"description\": \"클라우드 LLM 서빙을 선택한다. 복잡한 질문, 높은 품질 요구, 대용량 컨텍스트가 필요한 경우 사용한다.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"reason\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"클라우드 서빙을 선택한 이유\"\n",
    "                    },\n",
    "                    \"confidence\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"선택 신뢰도 (0.0-1.0)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"reason\", \"confidence\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"select_local_serving\",\n",
    "            \"description\": \"로컬 LLM 서빙을 선택한다. 단순한 질문, 빠른 응답, 프라이버시 보호, 비용 절감이 중요한 경우 사용한다.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"reason\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"로컬 서빙을 선택한 이유\"\n",
    "                    },\n",
    "                    \"confidence\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"선택 신뢰도 (0.0-1.0)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"reason\", \"confidence\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# 도구 이름과 엔드포인트 ID 매핑\n",
    "TOOL_TO_ENDPOINT_MAP = {\n",
    "    \"select_cloud_serving\": \"cloud_gpt4o_mini\",\n",
    "    \"select_local_serving\": \"local_ollama_llama3\"\n",
    "}\n",
    "\n",
    "print(f\"총 {len(SERVING_SELECTION_TOOLS)}개의 서빙 선택 도구가 정의되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 서빙 라우터 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서빙 라우터가 초기화되었다.\n"
     ]
    }
   ],
   "source": [
    "class ServingRouter:\n",
    "    \"\"\"LLM 서빙 라우터 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: ServingMemory):\n",
    "        self.memory = memory\n",
    "        self.router_model = \"gpt-4o-mini\"\n",
    "    \n",
    "    def route(self, query: str, characteristics: RequestCharacteristics) -> ServingSelection:\n",
    "        \"\"\"\n",
    "        요청 특성을 바탕으로 최적의 서빙을 선택한다\n",
    "        \n",
    "        Args:\n",
    "            query: 사용자 질문\n",
    "            characteristics: 요청 특성 분석 결과\n",
    "        \n",
    "        Returns:\n",
    "            ServingSelection 객체\n",
    "        \"\"\"\n",
    "        # 서비스별 과거 성능 정보 수집\n",
    "        cloud_perf = self.memory.get_service_performance(\"cloud_gpt4o_mini\")\n",
    "        local_perf = self.memory.get_service_performance(\"local_ollama_llama3\")\n",
    "        \n",
    "        routing_prompt = f\"\"\"\n",
    "커피 키오스크 질문에 대해 최적의 LLM 서빙을 선택하라.\n",
    "\n",
    "질문: \"{query}\"\n",
    "\n",
    "요청 특성:\n",
    "- 복잡도: {characteristics.complexity}\n",
    "- 프라이버시 민감도: {characteristics.privacy_sensitivity}\n",
    "- 응답 시간 요구: {characteristics.response_time_requirement}\n",
    "- 민감 정보 포함: {characteristics.contains_sensitive_data}\n",
    "- 예상 토큰: {characteristics.estimated_tokens}\n",
    "- 분석 근거: {characteristics.reasoning}\n",
    "\n",
    "사용 가능한 서빙:\n",
    "\n",
    "1. Cloud Serving (OpenAI GPT-4o-mini):\n",
    "   - 비용: $0.00030/1K tokens\n",
    "   - 평균 지연: 800ms\n",
    "   - 과거 성공률: {cloud_perf.get('success_rate', 0):.1%}\n",
    "   - 과거 평균 지연: {cloud_perf.get('avg_latency_ms', 0):.0f}ms\n",
    "   - 장점: 높은 품질, 복잡한 추론, 대용량 컨텍스트\n",
    "   - 단점: 비용 발생, 데이터 외부 전송\n",
    "\n",
    "2. Local Serving (Ollama Llama3):\n",
    "   - 비용: $0 (무료)\n",
    "   - 평균 지연: 300ms\n",
    "   - 과거 성공률: {local_perf.get('success_rate', 0):.1%}\n",
    "   - 과거 평균 지연: {local_perf.get('avg_latency_ms', 0):.0f}ms\n",
    "   - 장점: 무료, 빠른 응답, 프라이버시 보호\n",
    "   - 단점: 제한된 성능\n",
    "\n",
    "선택 기준:\n",
    "1. 프라이버시 민감도가 high이면 Local 우선\n",
    "2. 복잡도가 complex이면 Cloud 우선\n",
    "3. 응답 시간이 fast이면 Local 우선\n",
    "4. 비용 효율을 고려\n",
    "\n",
    "가장 적합한 서빙을 선택하는 함수를 호출하라.\n",
    "\"\"\"\n",
    "        \n",
    "        # Function Calling으로 서빙 선택\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.router_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": routing_prompt}],\n",
    "            tools=SERVING_SELECTION_TOOLS,\n",
    "            tool_choice=\"required\"\n",
    "        )\n",
    "        \n",
    "        # 도구 호출 결과 파싱\n",
    "        tool_call = response.choices[0].message.tool_calls[0]\n",
    "        function_name = tool_call.function.name\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        # 선택된 엔드포인트 가져오기\n",
    "        selected_endpoint_id = TOOL_TO_ENDPOINT_MAP[function_name]\n",
    "        selected_endpoint = SERVING_ENDPOINTS[selected_endpoint_id]\n",
    "        \n",
    "        # 대체 엔드포인트 설정\n",
    "        fallback_endpoint_id = None\n",
    "        if selected_endpoint.serving_type == ServingType.CLOUD:\n",
    "            fallback_endpoint_id = \"local_ollama_llama3\"\n",
    "        else:\n",
    "            fallback_endpoint_id = \"cloud_gpt4o_mini\"\n",
    "        fallback_endpoint = SERVING_ENDPOINTS[fallback_endpoint_id]\n",
    "        \n",
    "        # 예상 비용 계산\n",
    "        estimated_cost = (characteristics.estimated_tokens / 1000) * selected_endpoint.cost_per_1k_tokens\n",
    "        \n",
    "        return ServingSelection(\n",
    "            selected_endpoint=selected_endpoint,\n",
    "            selection_reason=function_args[\"reason\"],\n",
    "            confidence=function_args[\"confidence\"],\n",
    "            fallback_endpoint=fallback_endpoint,\n",
    "            estimated_cost=estimated_cost\n",
    "        )\n",
    "\n",
    "# 서빙 라우터 인스턴스 생성\n",
    "serving_router = ServingRouter(memory)\n",
    "print(\"서빙 라우터가 초기화되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LLM 서빙 클라이언트 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 서빙 클라이언트가 초기화되었다.\n",
      "  - Ollama 사용 가능: 예\n"
     ]
    }
   ],
   "source": [
    "class LLMServingClient:\n",
    "    \"\"\"LLM 서빙 클라이언트 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ollama_available = self._check_ollama_availability()\n",
    "    \n",
    "    def _check_ollama_availability(self) -> bool:\n",
    "        \"\"\"Ollama 로컬 서버가 사용 가능한지 확인한다\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=2)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def generate_cloud(self, query: str, context: List[Dict], endpoint: ServingEndpoint) -> tuple[str, int, int]:\n",
    "        \"\"\"\n",
    "        클라우드 LLM으로 응답을 생성한다\n",
    "        \n",
    "        Args:\n",
    "            query: 사용자 질문\n",
    "            context: 대화 컨텍스트\n",
    "            endpoint: 서빙 엔드포인트\n",
    "        \n",
    "        Returns:\n",
    "            (응답 내용, 지연시간 ms, 토큰 수)\n",
    "        \"\"\"\n",
    "        messages = context.copy()\n",
    "        messages.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=endpoint.model_name,\n",
    "            messages=messages,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        latency_ms = int((end_time - start_time) * 1000)\n",
    "        \n",
    "        content = response.choices[0].message.content.strip()\n",
    "        tokens_used = response.usage.total_tokens\n",
    "        \n",
    "        return content, latency_ms, tokens_used\n",
    "    \n",
    "    def generate_local(self, query: str, context: List[Dict], endpoint: ServingEndpoint) -> tuple[str, int, int]:\n",
    "        \"\"\"\n",
    "        로컬 LLM으로 응답을 생성한다\n",
    "        \n",
    "        Args:\n",
    "            query: 사용자 질문\n",
    "            context: 대화 컨텍스트\n",
    "            endpoint: 서빙 엔드포인트\n",
    "        \n",
    "        Returns:\n",
    "            (응답 내용, 지연시간 ms, 토큰 수)\n",
    "        \"\"\"\n",
    "        # Ollama가 사용 불가능하면 예외 발생\n",
    "        if not self.ollama_available:\n",
    "            raise Exception(\"Ollama 로컬 서버를 사용할 수 없다\")\n",
    "        \n",
    "        # 컨텍스트를 Ollama 형식으로 변환\n",
    "        prompt = self._build_ollama_prompt(query, context)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Ollama API 호출\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_BASE_URL}/api/generate\",\n",
    "            json={\n",
    "                \"model\": endpoint.model_name,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        latency_ms = int((end_time - start_time) * 1000)\n",
    "        \n",
    "        result = response.json()\n",
    "        content = result[\"response\"].strip()\n",
    "        \n",
    "        # Ollama는 토큰 수를 반환하지 않으므로 대략 추정\n",
    "        tokens_used = len(content.split()) * 2  # 간단한 추정\n",
    "        \n",
    "        return content, latency_ms, tokens_used\n",
    "    \n",
    "    def _build_ollama_prompt(self, query: str, context: List[Dict]) -> str:\n",
    "        \"\"\"Ollama용 프롬프트를 구성한다\"\"\"\n",
    "        prompt_parts = []\n",
    "        \n",
    "        for msg in context:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"]\n",
    "            \n",
    "            if role == \"system\":\n",
    "                prompt_parts.append(f\"시스템: {content}\")\n",
    "            elif role == \"user\":\n",
    "                prompt_parts.append(f\"사용자: {content}\")\n",
    "            elif role == \"assistant\":\n",
    "                prompt_parts.append(f\"도우미: {content}\")\n",
    "        \n",
    "        prompt_parts.append(f\"사용자: {query}\")\n",
    "        prompt_parts.append(\"도우미:\")\n",
    "        \n",
    "        return \"\\n\\n\".join(prompt_parts)\n",
    "    \n",
    "    def generate_mock_local(self, query: str, context: List[Dict], endpoint: ServingEndpoint) -> tuple[str, int, int]:\n",
    "        \"\"\"\n",
    "        Ollama가 없을 때 사용하는 모의 로컬 응답 생성\n",
    "        (실제로는 gpt-4o-mini를 사용하지만 로컬처럼 시뮬레이션)\n",
    "        \n",
    "        Returns:\n",
    "            (응답 내용, 지연시간 ms, 토큰 수)\n",
    "        \"\"\"\n",
    "        print(\"  [시뮬레이션] Ollama를 사용할 수 없어 모의 로컬 응답을 생성한다\")\n",
    "        \n",
    "        messages = context.copy()\n",
    "        messages.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 간단한 응답 생성 (로컬 모델은 더 간결함)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=0.5,\n",
    "            max_tokens=150  # 로컬 모델은 응답이 더 짧음\n",
    "        )\n",
    "        \n",
    "        # 로컬 모델의 더 빠른 응답 시간을 시뮬레이션\n",
    "        time.sleep(0.1)  # 로컬의 빠른 응답 시뮬레이션\n",
    "        \n",
    "        end_time = time.time()\n",
    "        latency_ms = int((end_time - start_time) * 1000)\n",
    "        \n",
    "        content = response.choices[0].message.content.strip()\n",
    "        tokens_used = response.usage.total_tokens\n",
    "        \n",
    "        return content, latency_ms, tokens_used\n",
    "\n",
    "# LLM 서빙 클라이언트 인스턴스 생성\n",
    "llm_client = LLMServingClient()\n",
    "print(f\"LLM 서빙 클라이언트가 초기화되었다.\")\n",
    "print(f\"  - Ollama 사용 가능: {'예' if llm_client.ollama_available else '아니오 (모의 로컬 모드 사용)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validation 시스템 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서빙 검증 시스템이 초기화되었다.\n"
     ]
    }
   ],
   "source": [
    "class ServingValidationSystem:\n",
    "    \"\"\"서빙 검증 시스템 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: LLMServingClient):\n",
    "        self.llm_client = llm_client\n",
    "        self.health_cache: Dict[str, ServiceHealth] = {}\n",
    "    \n",
    "    def check_service_health(self, endpoint: ServingEndpoint) -> ServiceHealth:\n",
    "        \"\"\"\n",
    "        서비스의 헬스를 체크한다\n",
    "        \n",
    "        Args:\n",
    "            endpoint: 체크할 엔드포인트\n",
    "        \n",
    "        Returns:\n",
    "            ServiceHealth 객체\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if endpoint.serving_type == ServingType.CLOUD:\n",
    "                # 클라우드는 항상 사용 가능하다고 가정\n",
    "                is_healthy = True\n",
    "                error_message = None\n",
    "            else:\n",
    "                # 로컬 서비스 체크\n",
    "                is_healthy = self.llm_client.ollama_available\n",
    "                error_message = None if is_healthy else \"Ollama 서버에 연결할 수 없다\"\n",
    "            \n",
    "            end_time = time.time()\n",
    "            response_time_ms = int((end_time - start_time) * 1000)\n",
    "            \n",
    "            health = ServiceHealth(\n",
    "                serving_type=endpoint.serving_type,\n",
    "                is_healthy=is_healthy,\n",
    "                response_time_ms=response_time_ms,\n",
    "                error_message=error_message\n",
    "            )\n",
    "            \n",
    "            # 캐시에 저장\n",
    "            cache_key = f\"{endpoint.serving_type.value}_{endpoint.model_name}\"\n",
    "            self.health_cache[cache_key] = health\n",
    "            \n",
    "            return health\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ServiceHealth(\n",
    "                serving_type=endpoint.serving_type,\n",
    "                is_healthy=False,\n",
    "                response_time_ms=None,\n",
    "                error_message=str(e)\n",
    "            )\n",
    "    \n",
    "    def validate_response(self, response_content: str, query: str) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        생성된 응답의 유효성을 검증한다\n",
    "        \n",
    "        Args:\n",
    "            response_content: 응답 내용\n",
    "            query: 원본 질문\n",
    "        \n",
    "        Returns:\n",
    "            (유효성, 검증 메시지)\n",
    "        \"\"\"\n",
    "        # 기본 검증\n",
    "        if not response_content or len(response_content.strip()) < 5:\n",
    "            return False, \"응답이 너무 짧거나 비어있다\"\n",
    "        \n",
    "        # 에러 메시지 체크\n",
    "        error_indicators = [\"오류\", \"에러\", \"실패\", \"불가능\"]\n",
    "        if any(indicator in response_content[:50] for indicator in error_indicators):\n",
    "            return False, \"응답에 오류 메시지가 포함되어 있다\"\n",
    "        \n",
    "        return True, \"응답이 유효하다\"\n",
    "\n",
    "# 검증 시스템 인스턴스 생성\n",
    "validation_system = ServingValidationSystem(llm_client)\n",
    "print(\"서빙 검증 시스템이 초기화되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Recovery 메커니즘 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서빙 복구 시스템이 초기화되었다.\n"
     ]
    }
   ],
   "source": [
    "class ServingRecoverySystem:\n",
    "    \"\"\"서빙 복구 메커니즘 시스템 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: LLMServingClient, validation_system: ServingValidationSystem):\n",
    "        self.llm_client = llm_client\n",
    "        self.validation_system = validation_system\n",
    "        self.fallback_history = []\n",
    "    \n",
    "    def attempt_with_fallback(\n",
    "        self,\n",
    "        query: str,\n",
    "        context: List[Dict],\n",
    "        selection: ServingSelection\n",
    "    ) -> tuple[ServingResponse, bool]:\n",
    "        \"\"\"\n",
    "        주 서비스로 시도하고 실패 시 대체 서비스로 폴백한다\n",
    "        \n",
    "        Args:\n",
    "            query: 사용자 질문\n",
    "            context: 대화 컨텍스트\n",
    "            selection: 서빙 선택 결과\n",
    "        \n",
    "        Returns:\n",
    "            (ServingResponse, 폴백 사용 여부)\n",
    "        \"\"\"\n",
    "        # 주 서비스 시도\n",
    "        primary_endpoint = selection.selected_endpoint\n",
    "        \n",
    "        print(f\"\\n주 서비스 시도: {primary_endpoint.endpoint_name}\")\n",
    "        \n",
    "        try:\n",
    "            response = self._generate_response(query, context, primary_endpoint)\n",
    "            \n",
    "            # 응답 검증\n",
    "            is_valid, validation_msg = self.validation_system.validate_response(\n",
    "                response.content,\n",
    "                query\n",
    "            )\n",
    "            \n",
    "            if is_valid:\n",
    "                print(f\"  성공: {validation_msg}\")\n",
    "                return response, False\n",
    "            else:\n",
    "                print(f\"  검증 실패: {validation_msg}\")\n",
    "                raise Exception(validation_msg)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  오류 발생: {str(e)}\")\n",
    "            \n",
    "            # 대체 서비스로 폴백\n",
    "            if selection.fallback_endpoint:\n",
    "                print(f\"\\n대체 서비스로 폴백: {selection.fallback_endpoint.endpoint_name}\")\n",
    "                \n",
    "                # 폴백 이력 기록\n",
    "                self.fallback_history.append({\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"from\": primary_endpoint.endpoint_name,\n",
    "                    \"to\": selection.fallback_endpoint.endpoint_name,\n",
    "                    \"reason\": str(e)\n",
    "                })\n",
    "                \n",
    "                try:\n",
    "                    response = self._generate_response(query, context, selection.fallback_endpoint)\n",
    "                    print(f\"  대체 서비스 성공\")\n",
    "                    response.was_fallback = True\n",
    "                    return response, True\n",
    "                    \n",
    "                except Exception as fallback_error:\n",
    "                    print(f\"  대체 서비스도 실패: {str(fallback_error)}\")\n",
    "                    raise Exception(f\"모든 서비스가 실패했다: {str(fallback_error)}\")\n",
    "            else:\n",
    "                raise Exception(f\"대체 서비스가 없다: {str(e)}\")\n",
    "    \n",
    "    def _generate_response(\n",
    "        self,\n",
    "        query: str,\n",
    "        context: List[Dict],\n",
    "        endpoint: ServingEndpoint\n",
    "    ) -> ServingResponse:\n",
    "        \"\"\"\n",
    "        지정된 엔드포인트로 응답을 생성한다\n",
    "        \n",
    "        Returns:\n",
    "            ServingResponse 객체\n",
    "        \"\"\"\n",
    "        # 서비스 헬스 체크\n",
    "        health = self.validation_system.check_service_health(endpoint)\n",
    "        if not health.is_healthy:\n",
    "            raise Exception(f\"서비스가 사용 불가능하다: {health.error_message}\")\n",
    "        \n",
    "        # 응답 생성\n",
    "        if endpoint.serving_type == ServingType.CLOUD:\n",
    "            content, latency_ms, tokens_used = self.llm_client.generate_cloud(\n",
    "                query, context, endpoint\n",
    "            )\n",
    "        else:\n",
    "            # Ollama가 사용 가능하면 실제 로컬, 아니면 모의 로컬\n",
    "            if self.llm_client.ollama_available:\n",
    "                content, latency_ms, tokens_used = self.llm_client.generate_local(\n",
    "                    query, context, endpoint\n",
    "                )\n",
    "            else:\n",
    "                content, latency_ms, tokens_used = self.llm_client.generate_mock_local(\n",
    "                    query, context, endpoint\n",
    "                )\n",
    "        \n",
    "        # 비용 계산\n",
    "        actual_cost = (tokens_used / 1000) * endpoint.cost_per_1k_tokens\n",
    "        \n",
    "        return ServingResponse(\n",
    "            content=content,\n",
    "            serving_type=endpoint.serving_type,\n",
    "            model_name=endpoint.model_name,\n",
    "            latency_ms=latency_ms,\n",
    "            tokens_used=tokens_used,\n",
    "            actual_cost=actual_cost,\n",
    "            was_fallback=False\n",
    "        )\n",
    "    \n",
    "    def get_fallback_stats(self) -> Dict:\n",
    "        \"\"\"폴백 통계를 반환한다\"\"\"\n",
    "        if not self.fallback_history:\n",
    "            return {\"count\": 0, \"most_common_reason\": None}\n",
    "        \n",
    "        reasons = [f[\"reason\"] for f in self.fallback_history]\n",
    "        most_common = max(set(reasons), key=reasons.count) if reasons else None\n",
    "        \n",
    "        return {\n",
    "            \"count\": len(self.fallback_history),\n",
    "            \"most_common_reason\": most_common\n",
    "        }\n",
    "\n",
    "# 복구 시스템 인스턴스 생성\n",
    "recovery_system = ServingRecoverySystem(llm_client, validation_system)\n",
    "print(\"서빙 복구 시스템이 초기화되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feedback 시스템 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서빙 피드백 시스템이 초기화되었다.\n"
     ]
    }
   ],
   "source": [
    "class ServingFeedbackSystem:\n",
    "    \"\"\"서빙 피드백 시스템 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, memory: ServingMemory):\n",
    "        self.memory = memory\n",
    "    \n",
    "    def analyze_service_efficiency(self) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        각 서비스의 효율성을 분석한다\n",
    "        \n",
    "        Returns:\n",
    "            서비스별 효율성 분석 결과\n",
    "        \"\"\"\n",
    "        efficiency = {}\n",
    "        \n",
    "        for endpoint_id, endpoint in SERVING_ENDPOINTS.items():\n",
    "            stats = self.memory.get_service_performance(endpoint_id)\n",
    "            \n",
    "            if stats[\"total_requests\"] == 0:\n",
    "                efficiency[endpoint_id] = {\n",
    "                    \"score\": 0.0,\n",
    "                    \"cost_efficiency\": 0.0,\n",
    "                    \"speed_score\": 0.0,\n",
    "                    \"reliability_score\": 0.0\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            # 비용 효율성 (낮을수록 좋음, 로컬은 만점)\n",
    "            if endpoint.cost_per_1k_tokens == 0:\n",
    "                cost_efficiency = 1.0\n",
    "            else:\n",
    "                cost_efficiency = max(0, 1.0 - (stats[\"total_cost\"] / 1.0))  # 1달러 기준\n",
    "            \n",
    "            # 속도 점수 (빠를수록 좋음)\n",
    "            target_latency = 500  # 목표 지연시간 (ms)\n",
    "            speed_score = max(0, 1.0 - (stats[\"avg_latency_ms\"] / target_latency))\n",
    "            \n",
    "            # 신뢰도 점수\n",
    "            reliability_score = stats[\"success_rate\"]\n",
    "            \n",
    "            # 종합 효율성 점수\n",
    "            overall_score = (\n",
    "                cost_efficiency * 0.3 +\n",
    "                speed_score * 0.3 +\n",
    "                reliability_score * 0.4\n",
    "            )\n",
    "            \n",
    "            efficiency[endpoint_id] = {\n",
    "                \"score\": overall_score,\n",
    "                \"cost_efficiency\": cost_efficiency,\n",
    "                \"speed_score\": speed_score,\n",
    "                \"reliability_score\": reliability_score\n",
    "            }\n",
    "        \n",
    "        return efficiency\n",
    "    \n",
    "    def generate_optimization_recommendations(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        성능 분석을 바탕으로 최적화 제안을 생성한다\n",
    "        \n",
    "        Returns:\n",
    "            최적화 제안 리스트\n",
    "        \"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # 서비스 효율성 분석\n",
    "        efficiency = self.analyze_service_efficiency()\n",
    "        \n",
    "        # 비용 분석\n",
    "        total_cost = self.memory.get_total_cost()\n",
    "        cost_savings = self.memory.get_cost_savings()\n",
    "        \n",
    "        if total_cost > 1.0:  # 1달러 이상 사용\n",
    "            recommendations.append(\n",
    "                f\"총 비용이 ${total_cost:.4f}다. \"\n",
    "                f\"로컬 서비스를 더 많이 사용하여 비용을 절감하라.\"\n",
    "            )\n",
    "        \n",
    "        if cost_savings > 0.5:  # 50센트 이상 절감\n",
    "            recommendations.append(\n",
    "                f\"로컬 서비스 사용으로 ${cost_savings:.4f}를 절감했다. \"\n",
    "                f\"현재 전략을 유지하라.\"\n",
    "            )\n",
    "        \n",
    "        # 서비스별 성능 분석\n",
    "        for endpoint_id, eff in efficiency.items():\n",
    "            endpoint = SERVING_ENDPOINTS[endpoint_id]\n",
    "            stats = self.memory.get_service_performance(endpoint_id)\n",
    "            \n",
    "            if stats[\"total_requests\"] == 0:\n",
    "                continue\n",
    "            \n",
    "            # 성공률이 낮은 경우\n",
    "            if eff[\"reliability_score\"] < 0.8:\n",
    "                recommendations.append(\n",
    "                    f\"{endpoint.endpoint_name}의 성공률이 {eff['reliability_score']:.1%}로 낮다. \"\n",
    "                    f\"서비스 상태를 점검하라.\"\n",
    "                )\n",
    "            \n",
    "            # 속도가 느린 경우\n",
    "            if eff[\"speed_score\"] < 0.5:\n",
    "                recommendations.append(\n",
    "                    f\"{endpoint.endpoint_name}의 응답 속도가 느리다 (평균 {stats['avg_latency_ms']:.0f}ms). \"\n",
    "                    f\"더 빠른 서비스를 우선 사용하라.\"\n",
    "                )\n",
    "        \n",
    "        # 로컬 서비스 활용도 분석\n",
    "        local_stats = self.memory.get_service_performance(\"local_ollama_llama3\")\n",
    "        cloud_stats = self.memory.get_service_performance(\"cloud_gpt4o_mini\")\n",
    "        \n",
    "        total_requests = local_stats[\"total_requests\"] + cloud_stats[\"total_requests\"]\n",
    "        \n",
    "        if total_requests > 0:\n",
    "            local_ratio = local_stats[\"total_requests\"] / total_requests\n",
    "            \n",
    "            if local_ratio < 0.3:\n",
    "                recommendations.append(\n",
    "                    f\"로컬 서비스 사용률이 {local_ratio:.1%}로 낮다. \"\n",
    "                    f\"간단한 질문에 로컬 서비스를 더 활용하여 비용을 절감하라.\"\n",
    "                )\n",
    "            elif local_ratio > 0.7:\n",
    "                recommendations.append(\n",
    "                    f\"로컬 서비스 사용률이 {local_ratio:.1%}로 높다. \"\n",
    "                    f\"복잡한 질문에는 클라우드 서비스를 사용하여 품질을 향상하라.\"\n",
    "                )\n",
    "        \n",
    "        return recommendations if recommendations else [\"현재 서빙 전략이 최적이다.\"]\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict:\n",
    "        \"\"\"성능 요약을 반환한다\"\"\"\n",
    "        cloud_stats = self.memory.get_service_performance(\"cloud_gpt4o_mini\")\n",
    "        local_stats = self.memory.get_service_performance(\"local_ollama_llama3\")\n",
    "        \n",
    "        return {\n",
    "            \"cloud\": {\n",
    "                \"requests\": cloud_stats[\"total_requests\"],\n",
    "                \"success_rate\": cloud_stats[\"success_rate\"],\n",
    "                \"avg_latency_ms\": cloud_stats[\"avg_latency_ms\"],\n",
    "                \"total_cost\": cloud_stats[\"total_cost\"]\n",
    "            },\n",
    "            \"local\": {\n",
    "                \"requests\": local_stats[\"total_requests\"],\n",
    "                \"success_rate\": local_stats[\"success_rate\"],\n",
    "                \"avg_latency_ms\": local_stats[\"avg_latency_ms\"],\n",
    "                \"total_cost\": local_stats[\"total_cost\"]\n",
    "            },\n",
    "            \"total_cost\": self.memory.get_total_cost(),\n",
    "            \"cost_savings\": self.memory.get_cost_savings()\n",
    "        }\n",
    "\n",
    "# 피드백 시스템 인스턴스 생성\n",
    "feedback_system = ServingFeedbackSystem(memory)\n",
    "print(\"서빙 피드백 시스템이 초기화되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Router LLM Serving Agent 통합 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Router LLM Serving Agent가 초기화되었다.\n"
     ]
    }
   ],
   "source": [
    "class RouterLLMServingAgent:\n",
    "    \"\"\"Router LLM Serving Agent 메인 클래스\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        request_analyzer: RequestAnalyzer,\n",
    "        serving_router: ServingRouter,\n",
    "        recovery_system: ServingRecoverySystem,\n",
    "        memory: ServingMemory,\n",
    "        feedback: ServingFeedbackSystem\n",
    "    ):\n",
    "        self.request_analyzer = request_analyzer\n",
    "        self.serving_router = serving_router\n",
    "        self.recovery_system = recovery_system\n",
    "        self.memory = memory\n",
    "        self.feedback = feedback\n",
    "    \n",
    "    def process_query(self, user_query: str) -> AgentResponse:\n",
    "        \"\"\"\n",
    "        사용자 질문을 처리하고 최종 응답을 생성한다\n",
    "        \n",
    "        Args:\n",
    "            user_query: 사용자 질문\n",
    "        \n",
    "        Returns:\n",
    "            AgentResponse 객체\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"사용자 질문: {user_query}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # 1. 사용자 메시지를 메모리에 추가\n",
    "        self.memory.add_message(\"user\", user_query)\n",
    "        \n",
    "        # 2. 요청 특성 분석\n",
    "        print(\"\\n단계 1: 요청 특성 분석\")\n",
    "        context = self.memory.get_context()\n",
    "        characteristics = self.request_analyzer.analyze_request(user_query, context)\n",
    "        print(f\"  - 복잡도: {characteristics.complexity}\")\n",
    "        print(f\"  - 프라이버시: {characteristics.privacy_sensitivity}\")\n",
    "        print(f\"  - 응답 시간: {characteristics.response_time_requirement}\")\n",
    "        print(f\"  - 민감 정보: {'포함' if characteristics.contains_sensitive_data else '없음'}\")\n",
    "        print(f\"  - 분석 근거: {characteristics.reasoning}\")\n",
    "        \n",
    "        # 3. 서빙 선택\n",
    "        print(\"\\n단계 2: 서빙 선택\")\n",
    "        selection = self.serving_router.route(user_query, characteristics)\n",
    "        print(f\"  - 선택된 서빙: {selection.selected_endpoint.endpoint_name}\")\n",
    "        print(f\"  - 선택 이유: {selection.selection_reason}\")\n",
    "        print(f\"  - 신뢰도: {selection.confidence:.2f}\")\n",
    "        print(f\"  - 예상 비용: ${selection.estimated_cost:.6f}\")\n",
    "        \n",
    "        # 4. 응답 생성 (폴백 포함)\n",
    "        print(\"\\n단계 3: 응답 생성\")\n",
    "        serving_response, was_fallback = self.recovery_system.attempt_with_fallback(\n",
    "            user_query,\n",
    "            context,\n",
    "            selection\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n응답 생성 완료:\")\n",
    "        print(f\"  - 사용된 서빙: {serving_response.serving_type.value}\")\n",
    "        print(f\"  - 모델: {serving_response.model_name}\")\n",
    "        print(f\"  - 지연시간: {serving_response.latency_ms}ms\")\n",
    "        print(f\"  - 토큰: {serving_response.tokens_used}개\")\n",
    "        print(f\"  - 비용: ${serving_response.actual_cost:.6f}\")\n",
    "        print(f\"  - 폴백 사용: {'예' if was_fallback else '아니오'}\")\n",
    "        \n",
    "        # 5. 서빙 응답 기록\n",
    "        self.memory.record_serving_response(serving_response, success=True)\n",
    "        \n",
    "        # 6. 비용 절감 계산\n",
    "        cost_savings = self.memory.get_cost_savings()\n",
    "        \n",
    "        # 7. 메모리에 답변 추가\n",
    "        self.memory.add_message(\n",
    "            \"assistant\",\n",
    "            serving_response.content,\n",
    "            serving_metadata={\n",
    "                \"serving_type\": serving_response.serving_type.value,\n",
    "                \"latency_ms\": serving_response.latency_ms,\n",
    "                \"cost\": serving_response.actual_cost\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 8. 사용자 추천 생성\n",
    "        recommendation = None\n",
    "        if serving_response.serving_type == ServingType.LOCAL:\n",
    "            recommendation = \"로컬 서비스를 사용하여 비용을 절감했다.\"\n",
    "        elif characteristics.privacy_sensitivity == \"high\":\n",
    "            recommendation = \"민감한 정보는 로컬 서비스 사용을 권장한다.\"\n",
    "        \n",
    "        # 9. 최종 응답 생성\n",
    "        return AgentResponse(\n",
    "            answer=serving_response.content,\n",
    "            serving_info=serving_response,\n",
    "            request_analysis=characteristics,\n",
    "            cost_savings=cost_savings,\n",
    "            recommendation=recommendation\n",
    "        )\n",
    "\n",
    "# Router LLM Serving Agent 인스턴스 생성\n",
    "agent = RouterLLMServingAgent(\n",
    "    request_analyzer=request_analyzer,\n",
    "    serving_router=serving_router,\n",
    "    recovery_system=recovery_system,\n",
    "    memory=memory,\n",
    "    feedback=feedback_system\n",
    ")\n",
    "\n",
    "print(\"\\nRouter LLM Serving Agent가 초기화되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 에이전트 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################################################\n",
      "테스트 1/4\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "사용자 질문: 안녕하세요\n",
      "============================================================\n",
      "\n",
      "단계 1: 요청 특성 분석\n",
      "  - 복잡도: simple\n",
      "  - 프라이버시: low\n",
      "  - 응답 시간: fast\n",
      "  - 민감 정보: 없음\n",
      "  - 분석 근거: 고객의 인사는 단순한 질문으로, 민감한 정보가 없고 즉각적인 응답이 필요하다.\n",
      "\n",
      "단계 2: 서빙 선택\n",
      "  - 선택된 서빙: Ollama exaone3.5\n",
      "  - 선택 이유: 요청이 단순하고 응답 시간이 빠르며 비용이 발생하지 않기 때문에 로컬 서빙을 선택함.\n",
      "  - 신뢰도: 0.90\n",
      "  - 예상 비용: $0.000000\n",
      "\n",
      "단계 3: 응답 생성\n",
      "\n",
      "주 서비스 시도: Ollama exaone3.5\n",
      "  오류 발생: 'response'\n",
      "\n",
      "대체 서비스로 폴백: OpenAI GPT-4o-mini\n",
      "  대체 서비스 성공\n",
      "\n",
      "응답 생성 완료:\n",
      "  - 사용된 서빙: cloud\n",
      "  - 모델: gpt-4o-mini\n",
      "  - 지연시간: 4174ms\n",
      "  - 토큰: 449개\n",
      "  - 비용: $0.000135\n",
      "  - 폴백 사용: 예\n",
      "\n",
      "============================================================\n",
      "최종 응답\n",
      "============================================================\n",
      "답변: {\n",
      "  \"complexity\": \"simple\",\n",
      "  \"privacy_sensitivity\": \"low\",\n",
      "  \"response_time_requirement\": \"fast\",\n",
      "  \"contains_sensitive_data\": false,\n",
      "  \"estimated_tokens\": 2,\n",
      "  \"reasoning\": \"단순한 인사말로, 복잡한 정보나 프라이버시 민감도가 없기 때문에 간단한 질문으로 분류됨.\"\n",
      "}\n",
      "\n",
      "서빙 정보:\n",
      "  - 타입: cloud\n",
      "  - 모델: gpt-4o-mini\n",
      "  - 지연시간: 4174ms\n",
      "  - 비용: $0.000135\n",
      "\n",
      "누적 비용 절감: $0.000000\n",
      "\n",
      "\n",
      "############################################################\n",
      "테스트 2/4\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "사용자 질문: 아메리카노 가격은 얼마인가요?\n",
      "============================================================\n",
      "\n",
      "단계 1: 요청 특성 분석\n",
      "  - 복잡도: simple\n",
      "  - 프라이버시: low\n",
      "  - 응답 시간: fast\n",
      "  - 민감 정보: 없음\n",
      "  - 분석 근거: 아메리카노 가격에 대한 단순한 질문으로, 복잡한 정보나 민감한 데이터가 포함되지 않음.\n",
      "\n",
      "단계 2: 서빙 선택\n",
      "  - 선택된 서빙: Ollama exaone3.5\n",
      "  - 선택 이유: 단순 질문으로 빠른 응답이 필요한 경우이므로 로컬 서빙이 최적이다.\n",
      "  - 신뢰도: 0.90\n",
      "  - 예상 비용: $0.000000\n",
      "\n",
      "단계 3: 응답 생성\n",
      "\n",
      "주 서비스 시도: Ollama exaone3.5\n",
      "  오류 발생: 'response'\n",
      "\n",
      "대체 서비스로 폴백: OpenAI GPT-4o-mini\n",
      "  대체 서비스 성공\n",
      "\n",
      "응답 생성 완료:\n",
      "  - 사용된 서빙: cloud\n",
      "  - 모델: gpt-4o-mini\n",
      "  - 지연시간: 1636ms\n",
      "  - 토큰: 567개\n",
      "  - 비용: $0.000170\n",
      "  - 폴백 사용: 예\n",
      "\n",
      "============================================================\n",
      "최종 응답\n",
      "============================================================\n",
      "답변: {\n",
      "  \"complexity\": \"simple\",\n",
      "  \"privacy_sensitivity\": \"low\",\n",
      "  \"response_time_requirement\": \"fast\",\n",
      "  \"contains_sensitive_data\": false,\n",
      "  \"estimated_tokens\": 7,\n",
      "  \"reasoning\": \"아메리카노의 가격에 대한 단순한 질문으로, 민감한 정보가 없고 즉각적인 응답이 필요한 질문이다.\"\n",
      "}\n",
      "\n",
      "서빙 정보:\n",
      "  - 타입: cloud\n",
      "  - 모델: gpt-4o-mini\n",
      "  - 지연시간: 1636ms\n",
      "  - 비용: $0.000170\n",
      "\n",
      "누적 비용 절감: $0.000000\n",
      "\n",
      "\n",
      "############################################################\n",
      "테스트 3/4\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "사용자 질문: 저는 카페인에 민감한데, 맛있는 음료를 추천해주세요\n",
      "============================================================\n",
      "\n",
      "단계 1: 요청 특성 분석\n",
      "  - 복잡도: moderate\n",
      "  - 프라이버시: medium\n",
      "  - 응답 시간: quality\n",
      "  - 민감 정보: 없음\n",
      "  - 분석 근거: 고객의 카페인 민감도를 고려한 맛있는 음료 추천 요청으로, 개인의 선호도 정보가 포함되어 있으며, 품질 있는 추천이 필요하다.\n",
      "\n",
      "단계 2: 서빙 선택\n",
      "  - 선택된 서빙: OpenAI GPT-4o-mini\n",
      "  - 선택 이유: 고객의 카페인 민감도를 고려한 맛있는 음료 추천 요청으로 복잡한 추론이 필요하고, 품질 있는 추천이 중요하기 때문입니다.\n",
      "  - 신뢰도: 0.95\n",
      "  - 예상 비용: $0.000004\n",
      "\n",
      "단계 3: 응답 생성\n",
      "\n",
      "주 서비스 시도: OpenAI GPT-4o-mini\n",
      "  성공: 응답이 유효하다\n",
      "\n",
      "응답 생성 완료:\n",
      "  - 사용된 서빙: cloud\n",
      "  - 모델: gpt-4o-mini\n",
      "  - 지연시간: 1876ms\n",
      "  - 토큰: 691개\n",
      "  - 비용: $0.000207\n",
      "  - 폴백 사용: 아니오\n",
      "\n",
      "============================================================\n",
      "최종 응답\n",
      "============================================================\n",
      "답변: {\n",
      "  \"complexity\": \"moderate\",\n",
      "  \"privacy_sensitivity\": \"medium\",\n",
      "  \"response_time_requirement\": \"quality\",\n",
      "  \"contains_sensitive_data\": false,\n",
      "  \"estimated_tokens\": 15,\n",
      "  \"reasoning\": \"카페인에 민감하다는 개인적인 선호를 고려한 음료 추천 요청으로, 다소 복잡한 응답이 필요하다.\"\n",
      "}\n",
      "\n",
      "서빙 정보:\n",
      "  - 타입: cloud\n",
      "  - 모델: gpt-4o-mini\n",
      "  - 지연시간: 1876ms\n",
      "  - 비용: $0.000207\n",
      "\n",
      "누적 비용 절감: $0.000000\n",
      "\n",
      "\n",
      "############################################################\n",
      "테스트 4/4\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "사용자 질문: 회의 전에 집중력을 높이면서도 오후까지 지속 가능한 에너지를 줄 수 있는 음료 조합을 추천해주세요. 단, 설탕은 적게 넣고 싶어요.\n",
      "============================================================\n",
      "\n",
      "단계 1: 요청 특성 분석\n",
      "  - 복잡도: complex\n",
      "  - 프라이버시: medium\n",
      "  - 응답 시간: quality\n",
      "  - 민감 정보: 없음\n",
      "  - 분석 근거: 회의 전 집중력과 지속 가능한 에너지를 고려한 맞춤형 음료 조합 요청으로, 설탕 선호도까지 포함되어 있어 복잡한 질문이다.\n",
      "\n",
      "단계 2: 서빙 선택\n",
      "  - 선택된 서빙: OpenAI GPT-4o-mini\n",
      "  - 선택 이유: 복잡한 질문으로, 회의 전 집중력과 지속 가능한 에너지에 대한 맞춤형 음료 조합의 높은 품질 응답이 필요하기 때문에 Cloud Serving을 선택합니다.\n",
      "  - 신뢰도: 0.90\n",
      "  - 예상 비용: $0.000009\n",
      "\n",
      "단계 3: 응답 생성\n",
      "\n",
      "주 서비스 시도: OpenAI GPT-4o-mini\n",
      "  성공: 응답이 유효하다\n",
      "\n",
      "응답 생성 완료:\n",
      "  - 사용된 서빙: cloud\n",
      "  - 모델: gpt-4o-mini\n",
      "  - 지연시간: 2036ms\n",
      "  - 토큰: 866개\n",
      "  - 비용: $0.000260\n",
      "  - 폴백 사용: 아니오\n",
      "\n",
      "============================================================\n",
      "최종 응답\n",
      "============================================================\n",
      "답변: {\n",
      "  \"complexity\": \"complex\",\n",
      "  \"privacy_sensitivity\": \"medium\",\n",
      "  \"response_time_requirement\": \"quality\",\n",
      "  \"contains_sensitive_data\": false,\n",
      "  \"estimated_tokens\": 30,\n",
      "  \"reasoning\": \"회의 전 집중력 향상과 지속 가능한 에너지를 원하며, 설탕을 적게 넣고 싶은 맞춤형 음료 조합 요청으로 복잡한 추천이 필요하다.\"\n",
      "}\n",
      "\n",
      "서빙 정보:\n",
      "  - 타입: cloud\n",
      "  - 모델: gpt-4o-mini\n",
      "  - 지연시간: 2036ms\n",
      "  - 비용: $0.000260\n",
      "\n",
      "누적 비용 절감: $0.000000\n"
     ]
    }
   ],
   "source": [
    "# 테스트 질문 리스트 (다양한 특성)\n",
    "test_queries = [\n",
    "    \"안녕하세요\",  # Simple, Local 예상\n",
    "    \"아메리카노 가격은 얼마인가요?\",  # Simple, Local 예상\n",
    "    \"저는 카페인에 민감한데, 맛있는 음료를 추천해주세요\",  # Moderate, 건강정보 포함\n",
    "    \"회의 전에 집중력을 높이면서도 오후까지 지속 가능한 에너지를 줄 수 있는 음료 조합을 추천해주세요. 단, 설탕은 적게 넣고 싶어요.\",  # Complex, Cloud 예상\n",
    "]\n",
    "\n",
    "# 각 질문에 대해 에이전트 실행\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n\\n{'#'*60}\")\n",
    "    print(f\"테스트 {i}/{len(test_queries)}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    response = agent.process_query(query)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"최종 응답\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"답변: {response.answer}\")\n",
    "    print(f\"\\n서빙 정보:\")\n",
    "    print(f\"  - 타입: {response.serving_info.serving_type.value}\")\n",
    "    print(f\"  - 모델: {response.serving_info.model_name}\")\n",
    "    print(f\"  - 지연시간: {response.serving_info.latency_ms}ms\")\n",
    "    print(f\"  - 비용: ${response.serving_info.actual_cost:.6f}\")\n",
    "    print(f\"\\n누적 비용 절감: ${response.cost_savings:.6f}\")\n",
    "    if response.recommendation:\n",
    "        print(f\"추천 사항: {response.recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 대화형 인터페이스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "커피 키오스크 Router LLM Serving Agent\n",
      "종료하려면 'quit' 또는 'exit'를 입력하세요.\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "질문:  라떼 3잔을 주문하고 싶어\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "사용자 질문: 라떼 3잔을 주문하고 싶어\n",
      "============================================================\n",
      "\n",
      "단계 1: 요청 특성 분석\n",
      "  - 복잡도: simple\n",
      "  - 프라이버시: low\n",
      "  - 응답 시간: fast\n",
      "  - 민감 정보: 없음\n",
      "  - 분석 근거: 단순한 주문 요청으로 복잡하지 않으며, 민감한 정보가 포함되어 있지 않기 때문에 즉각적인 응답이 필요하다.\n",
      "\n",
      "단계 2: 서빙 선택\n",
      "  - 선택된 서빙: Ollama exaone3.5\n",
      "  - 선택 이유: 단순한 주문 요청이므로 빠른 응답이 필요하고, 비용이 발생하지 않으며 프라이버시를 보호할 수 있다.\n",
      "  - 신뢰도: 0.90\n",
      "  - 예상 비용: $0.000000\n",
      "\n",
      "단계 3: 응답 생성\n",
      "\n",
      "주 서비스 시도: Ollama exaone3.5\n",
      "  오류 발생: 'response'\n",
      "\n",
      "대체 서비스로 폴백: OpenAI GPT-4o-mini\n",
      "  대체 서비스 성공\n",
      "\n",
      "응답 생성 완료:\n",
      "  - 사용된 서빙: cloud\n",
      "  - 모델: gpt-4o-mini\n",
      "  - 지연시간: 1834ms\n",
      "  - 토큰: 908개\n",
      "  - 비용: $0.000272\n",
      "  - 폴백 사용: 예\n",
      "\n",
      "에이전트: {\n",
      "  \"complexity\": \"simple\",\n",
      "  \"privacy_sensitivity\": \"low\",\n",
      "  \"response_time_requirement\": \"fast\",\n",
      "  \"contains_sensitive_data\": false,\n",
      "  \"estimated_tokens\": 10,\n",
      "  \"reasoning\": \"단순한 주문 요청으로, 복잡한 정보나 민감한 데이터가 포함되어 있지 않기 때문입니다.\"\n",
      "}\n",
      "\n",
      "[cloud | 1834ms | $0.000272]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "질문:  나의 이름은 고우주이야\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "사용자 질문: 나의 이름은 고우주이야\n",
      "============================================================\n",
      "\n",
      "단계 1: 요청 특성 분석\n",
      "  - 복잡도: simple\n",
      "  - 프라이버시: low\n",
      "  - 응답 시간: fast\n",
      "  - 민감 정보: 없음\n",
      "  - 분석 근거: 단순히 자신의 이름을 언급하는 내용으로, 복잡한 질문이나 민감한 정보가 포함되어 있지 않기 때문입니다.\n",
      "\n",
      "단계 2: 서빙 선택\n",
      "  - 선택된 서빙: Ollama exaone3.5\n",
      "  - 선택 이유: 단순한 질문으로 빠른 응답과 비용 절감이 중요하기 때문에 로컬 서빙을 선택함.\n",
      "  - 신뢰도: 0.90\n",
      "  - 예상 비용: $0.000000\n",
      "\n",
      "단계 3: 응답 생성\n",
      "\n",
      "주 서비스 시도: Ollama exaone3.5\n",
      "  오류 발생: 'response'\n",
      "\n",
      "대체 서비스로 폴백: OpenAI GPT-4o-mini\n",
      "  대체 서비스 성공\n",
      "\n",
      "응답 생성 완료:\n",
      "  - 사용된 서빙: cloud\n",
      "  - 모델: gpt-4o-mini\n",
      "  - 지연시간: 1620ms\n",
      "  - 토큰: 1000개\n",
      "  - 비용: $0.000300\n",
      "  - 폴백 사용: 예\n",
      "\n",
      "에이전트: {\n",
      "  \"complexity\": \"simple\",\n",
      "  \"privacy_sensitivity\": \"low\",\n",
      "  \"response_time_requirement\": \"fast\",\n",
      "  \"contains_sensitive_data\": false,\n",
      "  \"estimated_tokens\": 8,\n",
      "  \"reasoning\": \"단순히 자신의 이름을 소개하는 내용으로, 복잡한 질문이 아니고 민감한 정보도 포함되어 있지 않습니다.\"\n",
      "}\n",
      "\n",
      "[cloud | 1620ms | $0.000300]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "질문:  라떼에 샷2잔, 우유 대신 두유, 라지 사이즈로 3잔 추가해줘\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "사용자 질문: 라떼에 샷2잔, 우유 대신 두유, 라지 사이즈로 3잔 추가해줘\n",
      "============================================================\n",
      "\n",
      "단계 1: 요청 특성 분석\n",
      "  - 복잡도: complex\n",
      "  - 프라이버시: low\n",
      "  - 응답 시간: fast\n",
      "  - 민감 정보: 없음\n",
      "  - 분석 근거: 주문 내용이 맞춤형 조합을 포함하고 있어 복잡하지만, 개인 정보는 포함되어 있지 않으며 즉각적인 응답이 필요하다.\n",
      "\n",
      "단계 2: 서빙 선택\n",
      "  - 선택된 서빙: OpenAI GPT-4o-mini\n",
      "  - 선택 이유: 주문 내용이 맞춤형 조합을 포함하고 있어 복잡하고, 높은 품질의 응답이 필요하다.\n",
      "  - 신뢰도: 0.90\n",
      "  - 예상 비용: $0.000006\n",
      "\n",
      "단계 3: 응답 생성\n",
      "\n",
      "주 서비스 시도: OpenAI GPT-4o-mini\n",
      "  성공: 응답이 유효하다\n",
      "\n",
      "응답 생성 완료:\n",
      "  - 사용된 서빙: cloud\n",
      "  - 모델: gpt-4o-mini\n",
      "  - 지연시간: 5224ms\n",
      "  - 토큰: 1144개\n",
      "  - 비용: $0.000343\n",
      "  - 폴백 사용: 아니오\n",
      "\n",
      "에이전트: {\n",
      "  \"complexity\": \"complex\",\n",
      "  \"privacy_sensitivity\": \"low\",\n",
      "  \"response_time_requirement\": \"fast\",\n",
      "  \"contains_sensitive_data\": false,\n",
      "  \"estimated_tokens\": 23,\n",
      "  \"reasoning\": \"주문 내용이 구체적이고 맞춤형 조합을 포함하고 있어 복잡한 주문 요청으로 평가됨.\"\n",
      "}\n",
      "\n",
      "[cloud | 5224ms | $0.000343]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "질문:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "대화를 종료한다.\n",
      "\n",
      "총 비용: $0.001687\n",
      "비용 절감: $0.000000\n"
     ]
    }
   ],
   "source": [
    "def chat_interface():\n",
    "    \"\"\"대화형 인터페이스 함수\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"커피 키오스크 Router LLM Serving Agent\")\n",
    "    print(\"종료하려면 'quit' 또는 'exit'를 입력하세요.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\n질문: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', '종료']:\n",
    "            print(\"\\n대화를 종료한다.\")\n",
    "            # 최종 통계 출력\n",
    "            summary = feedback_system.get_performance_summary()\n",
    "            print(f\"\\n총 비용: ${summary['total_cost']:.6f}\")\n",
    "            print(f\"비용 절감: ${summary['cost_savings']:.6f}\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        response = agent.process_query(user_input)\n",
    "        print(f\"\\n에이전트: {response.answer}\")\n",
    "        print(f\"\\n[{response.serving_info.serving_type.value} | \"\n",
    "              f\"{response.serving_info.latency_ms}ms | \"\n",
    "              f\"${response.serving_info.actual_cost:.6f}]\")\n",
    "\n",
    "# 대화 시작 (주석을 제거하여 실행)\n",
    "chat_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. 성능 분석 및 리포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "성능 분석 리포트\n",
      "============================================================\n",
      "\n",
      "1. 전체 통계\n",
      "  총 비용: $0.001687\n",
      "  비용 절감: $0.000000\n",
      "\n",
      "2. Cloud 서비스 (OpenAI GPT-4o-mini)\n",
      "  - 요청 수: 7회\n",
      "  - 성공률: 100.0%\n",
      "  - 평균 지연시간: 2629ms\n",
      "  - 총 비용: $0.001687\n",
      "\n",
      "3. Local 서비스 (Ollama Llama3)\n",
      "  - 요청 수: 0회\n",
      "  - 성공률: 0.0%\n",
      "  - 평균 지연시간: 0ms\n",
      "  - 총 비용: $0.000000\n",
      "\n",
      "4. 서비스 효율성 분석\n",
      "\n",
      "  OpenAI GPT-4o-mini:\n",
      "    - 종합 점수: 0.70\n",
      "    - 비용 효율성: 1.00\n",
      "    - 속도 점수: 0.00\n",
      "    - 신뢰도 점수: 1.00\n",
      "\n",
      "  Ollama exaone3.5:\n",
      "    - 종합 점수: 0.00\n",
      "    - 비용 효율성: 0.00\n",
      "    - 속도 점수: 0.00\n",
      "    - 신뢰도 점수: 0.00\n",
      "\n",
      "5. 폴백 통계\n",
      "  - 폴백 발생 횟수: 4회\n",
      "  - 주요 원인: 'response'\n",
      "\n",
      "6. 최적화 제안\n",
      "  1. OpenAI GPT-4o-mini의 응답 속도가 느리다 (평균 2629ms). 더 빠른 서비스를 우선 사용하라.\n",
      "  2. 로컬 서비스 사용률이 0.0%로 낮다. 간단한 질문에 로컬 서비스를 더 활용하여 비용을 절감하라.\n",
      "\n",
      "7. 메모리 상태\n",
      "  - 총 요청: 7회, 비용: $0.0017, 절감: $0.0000\n",
      "\n",
      "============================================================\n",
      "튜토리얼 4: Router LLM Serving Agent 완료\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"성능 분석 리포트\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 전체 통계\n",
    "performance_summary = feedback_system.get_performance_summary()\n",
    "\n",
    "print(\"\\n1. 전체 통계\")\n",
    "print(f\"  총 비용: ${performance_summary['total_cost']:.6f}\")\n",
    "print(f\"  비용 절감: ${performance_summary['cost_savings']:.6f}\")\n",
    "\n",
    "# 2. Cloud 서비스 성능\n",
    "cloud_perf = performance_summary['cloud']\n",
    "print(\"\\n2. Cloud 서비스 (OpenAI GPT-4o-mini)\")\n",
    "print(f\"  - 요청 수: {cloud_perf['requests']}회\")\n",
    "print(f\"  - 성공률: {cloud_perf['success_rate']:.1%}\")\n",
    "print(f\"  - 평균 지연시간: {cloud_perf['avg_latency_ms']:.0f}ms\")\n",
    "print(f\"  - 총 비용: ${cloud_perf['total_cost']:.6f}\")\n",
    "\n",
    "# 3. Local 서비스 성능\n",
    "local_perf = performance_summary['local']\n",
    "print(\"\\n3. Local 서비스 (Ollama Llama3)\")\n",
    "print(f\"  - 요청 수: {local_perf['requests']}회\")\n",
    "print(f\"  - 성공률: {local_perf['success_rate']:.1%}\")\n",
    "print(f\"  - 평균 지연시간: {local_perf['avg_latency_ms']:.0f}ms\")\n",
    "print(f\"  - 총 비용: ${local_perf['total_cost']:.6f}\")\n",
    "\n",
    "# 4. 서비스 효율성 분석\n",
    "print(\"\\n4. 서비스 효율성 분석\")\n",
    "efficiency = feedback_system.analyze_service_efficiency()\n",
    "for endpoint_id, eff in efficiency.items():\n",
    "    endpoint = SERVING_ENDPOINTS[endpoint_id]\n",
    "    print(f\"\\n  {endpoint.endpoint_name}:\")\n",
    "    print(f\"    - 종합 점수: {eff['score']:.2f}\")\n",
    "    print(f\"    - 비용 효율성: {eff['cost_efficiency']:.2f}\")\n",
    "    print(f\"    - 속도 점수: {eff['speed_score']:.2f}\")\n",
    "    print(f\"    - 신뢰도 점수: {eff['reliability_score']:.2f}\")\n",
    "\n",
    "# 5. 폴백 통계\n",
    "print(\"\\n5. 폴백 통계\")\n",
    "fallback_stats = recovery_system.get_fallback_stats()\n",
    "print(f\"  - 폴백 발생 횟수: {fallback_stats['count']}회\")\n",
    "if fallback_stats['most_common_reason']:\n",
    "    print(f\"  - 주요 원인: {fallback_stats['most_common_reason']}\")\n",
    "\n",
    "# 6. 최적화 제안\n",
    "print(\"\\n6. 최적화 제안\")\n",
    "recommendations = feedback_system.generate_optimization_recommendations()\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "# 7. 메모리 상태\n",
    "print(\"\\n7. 메모리 상태\")\n",
    "print(f\"  - {memory.get_summary()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"튜토리얼 4: Router LLM Serving Agent 완료\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
